\documentclass[10pt,journal,compsoc]{IEEEtran}
\usepackage{wrapfig}
 \usepackage{amsmath}
 \usepackage{url}
 \usepackage{pifont}
\usepackage{rotating}
%\usepackage{balance} 
\usepackage{color, colortbl}
\usepackage{graphicx}
\usepackage{algorithmicx}
\usepackage{program}
\usepackage{cite}
\usepackage{alltt}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\tion}[1]{\textsection\ref{sec:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\definecolor{lightgray}{gray}{0.975}
\usepackage{fancyvrb}
\usepackage{stfloats}
\usepackage{multirow}
\usepackage{listings}
\usepackage{amsmath} 
\DeclareMathOperator*{\argmin}{arg\,min} 
\DeclareMathOperator*{\argmax}{arg\,max} 


\definecolor{darkgreen}{rgb}{0,0.3,0}

\usepackage[table]{xcolor}
\definecolor{Gray}{rgb}{0.88,1,1}
\definecolor{Gray}{gray}{0.85}
\definecolor{Blue}{RGB}{0,29,193}
\newcommand{\G}{\cellcolor{green}}
\newcommand{\Y}{\cellcolor{yellow}}


\definecolor{MyDarkBlue}{rgb}{0,0.08,0.45} 
\newenvironment{changed}{\par\color{MyDarkBlue}}{\par}
%\newenvironment{changed}{\par}{\par}

\newcommand{\ADD}[1]{\textcolor{MyDarkBlue}{{\bf #1}}}
%\newcommand{\ADD}[1]{#1}

\newcommand{\addit}[1]{\begin{changed}\input{#1}\end{changed}}

\begin{document}

\title{GALE: Geometric Active Learning \\ for Search-Based Software Engineering}

\author{
Joseph~Krall\thanks{Joseph Krall is a postdoc at LoadIQ, Nevada, USA; email:  kralljoe@gmail.com.},
Tim~Menzies,~\IEEEmembership{Member,~IEEE},\thanks{
Tim Menzies
is with Computer Science, North Carolina State University, USA; e-mail: tim.menzies@gmail.com.}
Misty Davies~\IEEEmembership{Member,~IEEE}
\thanks{
Misty Davies is with the Intelligent Systems Division,
NASA Ames Research Center, CA, USA;
e-mail:
misty.d.davies@nasa.gov.}
}

\IEEEcompsoctitleabstractindextext{%
\begin{abstract}
Multi-objective evolutionary algorithms (MOEAs)
help software engineers
find novel solutions to complex problems. When
automatic tools explore too many options, they are 
slow to use and  hard to comprehend.
GALE is a  near-linear time MOEA that builds
a piecewise approximation 
to the surface of best solutions along the Pareto frontier.
For each piece,
GALE mutates solutions towards the better end.
In numerous case studies, 
GALE finds comparable solutions to standard methods
(NSGA-II, SPEA2) using far fewer evaluations (e.g.
20 evaluations, not 1000).
GALE is recommended when a
 model is expensive to evaluate, or when
some audience needs to browse and understand
how an MOEA has made its conclusions.

\end{abstract}

\begin{keywords}
Multi-objective optimization, Search based software engineering, Active Learning
\end{keywords}}

\maketitle

\markboth{IEEE Trans SE,~ Submitted Jan'14, Revision\#3, Apr'15}%
{Krall \MakeLowercase{\textit{et al.}}: Accelerated Active Learning for Search Based Software Engineering}


\IEEEdisplaynotcompsoctitleabstractindextext
\IEEEpeerreviewmaketitle


 
\section{Introduction}

In  traditional manual software engineering, engineers laboriously convert (by hand) non-executable paper models into executable code. 
That traditional process has been the focus of much research.  
This paper is about a new kind of SE which relies, at least in part, on executable models. 
In this approach, engineers codify the current understanding of the domain into a model, and then study those models.

Many of these models are delivered as part of working systems.
So much so that these models now
  mediate nearly all aspects of our lives:
\bi
\item If you
  live in London or New York and need to call an
  ambulance, that ambulance is waiting for your call
  at a location pre-determined by a model~\cite{veer11}. 
\item
If you cross from Mexico to Arizona,
a biometrics model  decides if you need
secondary screening~\cite{Sacanamboy09}.
\item
 The power to make your toast comes from a
  generator that was spun-up in response
  to some model predicting your future electrical
  demands~\cite{808235}.  
\item
If you fly a plane, extensive
  model-based software controls many aspects of
  flight, including what to do in emergency
  situations~\cite{Kim2011}. \item
If you have a heart attack, the
   models in the defibrillator will
  decide how to shock your heart and lungs so that
  you might live a little longer~\cite{kamp99}.
\ei

Given recent advances in computing hardware, software analysts either validate these models or find optimal solutions by using automatic tools to explore thousands to millions of inputs for their systems. 
%Such tools work at different speeds than humans.  
Valerdi notes that, without automated tools, it can take days for human experts to review just a few dozen examples~\cite{valerdi11}.  
In that same time, an automatic tool can explore thousands to millions to billions more solutions.  
People find it an overwhelming task just to certify the correctness of conclusions generated from so many results. 
Verrappa and Letier warn that
\begin{quote}
``..for industrial problems, these algorithms generate
(many) solutions, which makes the tasks of
understanding them and selecting one among them
difficult and time consuming''~\cite{veer11}.
\end{quote}
%% This is a problem since for many human-in-the-loop situations,
%% tools must be designed such that managers can debate their results
%% (e.g. to decide if it is wise
%% to deploy the results as a change to their
%% processes). 
%% \ADD{In our experience, 
%% to better support debates during decision making,
%% it is wise to
%% reducing the number of options business users
%% have to consider.}

%% \begin{figure}[!b]
%% \includegraphics[width=3.5in]{figures/amus.png}
%% \caption{Exploring options for configuring London
%% Ambulances~\cite{veer11} (to minimize $X$ and maximize $Y$).
%% Thousands of options, shown in red, have been rejected
%% to isolate clusters of better solutions C1,C2,etc. }\label{fig:one}
%% \end{figure}

One way to simplify the task of understanding the space of possible solutions is to focus on the {\em Pareto frontier}; i.e. the subset of solutions that are not worse than any other (across all goals) but better on at least one goal. 
%% For example,
%% the green dots
%% in \fig{one} are an example of such a Pareto
%% frontier (and the red dots are called the {\em dominated} examples).
The problem here is that even the Pareto frontier can be too large to understand.
Harman cautions that many frontiers are very {\em crowded}; i.e. contain thousands (or more)  candidate solutions~\cite{harm13}.
Hence, researchers like Verrappa and Letier add post-processors that (a)~cluster the Pareto frontier and then (b)~show users a small number of examples per cluster. 

That approach has the drawback that before the users can get their explanation, some other process must generate the Pareto frontier---which can be a very slow computation.  
Zuluaga et al. comment on the cost of such an analysis for software/hardware co-design: ``synthesis of only one design can take hours or even days.''~\cite{Zuluaga:13}.  
Harman~\cite{harm13} comments on the problems of evolving a test suite for software if every candidate solution requires a time-consuming execution of the entire system: such test suite generation can take weeks of execution time. 

For such slow computational problems, it would be useful to reason about a problem using a very small number of most informative examples. 
This paper introduces GALE, an optimizer that  identifies and evaluates just those most informative examples.
Note that GALE's approach is different from that of Verrappa \& Letier: GALE does not use clustering as a post-process to some other optimizer.
Rather, GALE {\em replaces} the need for a post-processor with its tool called WHERE, which explores only two evaluations per recursive split of the data. 
Hence, this algorithm performs at most $2{\log_2}(N)$ evaluations per generation, and often less. 

This paper introduces GALE and its algorithms and answers two key research questions for the SE-based problems explored in this paper.
\begin{quote}
{\bf RQ1 (speed):} Does GALE terminate faster than other multi-goal optimization tools?
\end{quote}
This is a concern since GALE must  repeatedly sort and divide  the examples---which might  make  GALE  slower than other multi-goal optimizers.

A second concern  is the quality of GALE's results:
\begin{quote}
{\bf RQ2 (quality):} Does GALE return similar or better solutions than other optimization tools?
\end{quote}
This is a concern since GALE only examines $2\log_2(N)$ of the solutions---which might mean that GALE misses useful optimizations found by other tools.

This paper is structured as follows.
After  notes on related work (in Section 2) we present the  details of GALE (in Section 3). 
The algorithm is  tested on a range of SE models of varying sizes, as described in Section 4.
 Section~5 offers some details on those tests.
 
 Section~6 shows the test results. In summary,
 regarding   {\bf RQ1 (speed)}, GALE ran much faster than other tools for our SE models, especially for those that were very large.
For example, in our largest model, GALE terminated in four minutes while other tools needed seven hours.
As to {\bf RQ2 (quality)}, we find that (as might be expected) GALE's truncated search sometimes explores a smaller  {\em hypervolume}  of solutions than other optimizers.
Yet within that smaller volume,   GALE's careful directed search is more {\em spread} out. 
More importantly, on inspection of  the raw objective scores, we often find better results with GALE than with other optimizers for our SE models of interest. 

 


\subsection{Availability}\label{sec:avail}

GALE is released under the GNU Lesser GPL and is available
as part of the JMOO package (Joe's multi-objective optimization), which
incorporates DEAP (Distributed Evolutionary Algorithms in Python~\cite{jmlr12}).
GALE and most of the models used here are available 
from \url{github.com/tanzairatier/jmoo} (and for the XOMO software process model, see
\url{github.com/nave91/modeller/tree/master/xomo}).


%% Lastly, even when humans are not in the loop for reasoning about a model, it is important for search-based
%% SE to reduce the number of evalxfwuations required for controlling a model. As software systems grow ever larger
%% and more complex, it becomes harder to explore all their internal effects. Hence, tools like GALE are required
%% to map out and better understand  the shape of those internal effects.


\newcommand{\Yes}{\ding{51}}
\newcommand{\No}{\ding{55}}


%\begin{figure}[!b]
%\color{MyDarkBlue}
%\b%% egin{center}%35
%% \scriptsize
%% \begin{tabular}{r@{~}|c@{~}|r@{~}|c}
%%      &                     &        Runtime,sec: avg of 100 &            \\
%%      &      Constrained    &   runs, random inputs           &      Ref      \\ \hline
%% BNH : d2-o2       &       \Yes     &       0.01    &       \cite{Binh97mobes:a}    \\      
%% Viennet3 : d2-o3  &       \No      &       0.01    &       \cite{viennetmodels}    \\      
%% Viennet2 : d2-o3  &       \No      &       0.01    &       \cite{viennetmodels}    \\      
%% Viennet4 : d2-o3  &       \No      &       0.01    &       \cite{viennetmodels}    \\      
%% TwoBarTruss : d3-o2       &       \Yes     &       0.01    &       \cite{Chafekar03constrainedmultiobjective}      \\      
%% Golinski : d7-o2  &       \No      &       0.01    &       \cite{golinskimodel}    \\      
%% Water : d3-o5     &       \Yes     &       0.02    &       \cite{watermodel}       \\      
%% ZDT6 : d10-o2     &       \No      &       0.02    &       \cite{Zitzler2000zdtpaper}      \\      
%% DTLZ1 : d5-o2     &       \No      &       0.02    &       \cite{dtlz2001a}        \\      
%% Srinivas : d2-o2  &       \Yes     &       0.03    &       \cite{DBLP:journals/ec/SrinivasD94}     \\      
%% ZDT4 : d10-o2     &       \No      &       0.03    &       \cite{Zitzler2000zdtpaper}      \\      
%% DTLZ5 : d10-o2    &       \No      &       0.03    &       \cite{dtlz2001a}        \\      
%% DTLZ2 : d10-o2    &       \No      &       0.03    &       \cite{dtlz2001a}        \\      
%% DTLZ3 : d10-o2    &       \No      &       0.03    &       \cite{dtlz2001a}        \\      
%% DTLZ4 : d10-o2    &       \No      &       0.03    &       \cite{dtlz2001a}        \\      
%% DTLZ2 : d10-o4    &       \No      &       0.03    &       \cite{dtlz2001a}        \\      
%% DTLZ1 : d5-o4     &       \No      &       0.04    &       \cite{dtlz2001a}        \\      
%% DTLZ4 : d10-o4    &       \No      &       0.04    &       \cite{dtlz2001a}        \\      
%% DTLZ3 : d10-o4    &       \No      &       0.04    &       \cite{dtlz2001a}        \\      
%% DTLZ5 : d10-o4    &       \No      &       0.04    &       \cite{dtlz2001a}        \\      
%% ZDT3 : d30-o2     &       \No      &       0.05    &       \cite{Zitzler2000zdtpaper}      \\      
%% ZDT1 : d30-o2     &       \No      &       0.05    &       \cite{Zitzler2000zdtpaper}      \\      
%% DTLZ6 : d20-o4    &       \No      &       0.05    &       \cite{dtlz2001a}        \\      
%% ZDT2 : d30-o2     &       \No      &       0.06    &       \cite{Zitzler2000zdtpaper}      \\      
%% DTLZ6 : d20-o2    &       \No      &       0.06    &       \cite{dtlz2001a}        \\      
%% Osyczka2 : d6-o2  &       \Yes     &       0.29    &       \cite{osymodel}         \\      
%% Tanaka : d2-o2    &       \Yes     &       0.35    &       \cite{537993}   \\      
%% xomogr : d27-o4   &       \No      &       0.96    &        \cite{me07f,me09a,me09e}       \\      
%% xomofl : d27-o4   &       \No      &       0.96    &         \cite{me07f,me09a,me09e}      \\      
%% xomoo2 : d27-o4   &       \No      &       0.99    &        \cite{me07f,me09a,me09e}       \\      
%% POM3B : d9-o4     &       \No      &       6.28    &       \cite{port08,me09j}     \\      
%% POM3A : d9-o4     &       \No      &       218.98  &       \cite{port08,me09j}     \\      
%% POM3C : d9-o4     &       \No      &       445.03  &       \cite{port08,me09j}     \\      
%% CDA     &       \No      &       8100.00    &       \cite{Kim2011,Pritchett2011,Feigh2012,Kim2013,Pritchett2013}   
%% \end{tabular}           
%% \end{center}
%% \caption{The 35 models used in this study. These come  from the 13 different references shown in the right-hand-side column.
%% The model name is specified in the following format:  \emph{name} - \emph{number of inputs} - \emph{number of objectives}.  For example, ``ZDT3 : d30-o2'' means ZDT3 with 30 decision inputs and 2 objective outputs.
%% For more details on these models, see \tion{models}.}\label{fig:35models}
%% \end{figure} 


\section{Related Work} 

%% \subsection{SBSE}


%% In software engineering (SE), it is often necessary to trade
%% off between many competing objectives.  This task is often handled
%% by search-based software engineering (SBSE) tools.
%% For example, Rodriguez et
%% al.~\cite{rod11} used a systems model of a software project~\cite{deb00afast}
%%  to search for inputs that generate favorable outputs---in this case, outputs that
%% most decrease time and cost while increasing productivity. That study
%% generated a three-dimensional surface model where managers could explore
%% trade-offs between the objectives of cost, timlooke and productivity.  In other
%% work, Heaven \& Letier~\cite{heaven11} studied a quantitative
%% goal graph model of the requirements of the London Ambulance
%% Services. The upper layers of that requirements model were a standard
%% Van Lamsweerde goal graph~\cite{lam00}, while the leaves drew their
%% values from statistical distributions. Using that model, they found
%% decisions that balance speed \& accuracy of ambulance-allocation
%% decisions. Subsequent work by Veerappa \& Letier (discussed in the introduction)
%% explored methods to better explain the generated set of solutions~\cite{veer11}.
%% For a list of other SBSE applications, see \fig{tasks}.

%In software engineering (SE), it is often necessary to trade
%% off between many competing objectives.  This task is often handled
%% by search-based software engineering (SBSE) tools.
%% Due to the complexity
%% of these tasks, exact optimization methods may be impractical.
%% Hence, researchers often resort to various meta-heuristic  approaches.
%% In the 1950s, when
%% computer RAM was very small, a standard technique was simulated
%% annealing (SA)~\cite{kirkpatrick83}. SA generates {\em new} solutions
%% by randomly perturbing (a.k.a. ``mutating'') some part of an {\em old}
%% solution.  {\em New} replaces {\em old} if (a) it scores higher; or
%% (b) it reaches some probability set by a ``temperature'' constant. Initially,
%% temperature is high so SA jumps to sub-optimal solutions (this allows
%% the algorithm to escape from local minima). Subsequently, the
%% ``temperature'' cools and SA only ever moves to better {\em new}
%% solutions.
%% SA is often used in SBSE
%% e.g.~\cite{fea02a,me07f,me07g}, perhaps due to its simplicity.

%% In the 1960s, when more RAM became available, it became standard to
%% generate many {\em new} mutants, and then combine together parts of
%% promising solutions~\cite{goldberg79}.  Such {\em evolutionary
%%   algorithms} (EA) work in {\em generations} over a population of
%% candidate solutions.  Initially, the population is created at random.
%% Subsequently, each generation makes use of select+crossover+mutate
%% operators to pick promising solutions, mix them up in some way, and
%% then slightly adjust them.
%% EAs are also often used in SBSE, particularly in test case generation; e.g.~\cite{andrews07,andrews10}.

%% Later work focused on creative ways to control the
%% mutation process. Tabu search and scatter search
%% work to bias new mutations away from prior
%% mutations~\cite{Glover1986563,Beausoleil2006426,Molina05sspmo:a,4455350}.
%% Differential evolution mutates solutions by
%% interpolating between members of the current
%% population~\cite{storn97}.  Particle swarm
%% optimization randomly mutates multiple solutions
%% (which are called ``particles''), but biases those
%% mutations towards the best solution seen by one
%% particle and/or by the neighborhood around that
%% particle~\cite{pan08}.
%% These methods are often used for parameter tuning for other learners.
%% For example, Cora et al.~\cite{cora10} use tabu search to learn the parameters
%% of a radial bias support vector machine for learning effort estimation models.

%% \begin{figure}[!t]
%% \small
%% \begin{tabular}{|p{3.1in}|}\hline
%% \bi
%% \item
%% {\em  Next release planning:}
%% Deliver most functionality
%% in least time and cost~\cite{zhang07a,bagnall01,DurilloZAHN11}.
%% \item {\em Risk management:} Maximize the covered requirements while minimizing the
%% cost of  mitigations applied  (to avoid possible problems)~\cite{fea02a,feather03,me08b}.
%% \item {\em Explore high-level designs:} Use most features
%% avoiding defects, with  least development time,  avoiding violations of
%% constraints~\cite{sayyad13a,sayyad13b}.
%% \item {\em Improve low-level designs:}
%% Apply automatic refactoring tools to object-oriented design in order improve the internal
%% cohesiveness of that design~\cite{ocinneide12}.
%% \item
%% {\em  Test case generation:} Adjust  test suites
%% to increase program  coverage by those tests~\cite{andrews07,andrews10,me09m}.
%% \item
%% {\em Regression tests:} Find tests that run fastest, with the most odds of being relevant
%% to  recently changed code, with the greatest probability of failing~\cite{weimer13}.
%% \item
%% {\em Cloud computing:}
%% For distributed CPUs and disks, adjust allocation and assignment of tasks
%% to  trade-off between availability,
%% performance and cost~\cite{harman13}.
%% \item
%% {\em Predictive Modeling:} Tune  data miner's parameters to improve  predictions of
%% software cost~\cite{cora10}.
%% \ei\\\hline
%% \end{tabular}
%% \caption{Some optimization tasks explored by SBSE.}\label{fig:tasks}
%% \end{figure} 
\subsection{Optimization}\label{sec:optz}
This paper is a comparative assessment of
GALE with some other  {\em optimizers}.
We  will argue that GALE is the preferred choice for
{\em functional optimization} when the {\em evaluation cost}
is very large.  

This section explains all the technical terms in the last paragraph.
To start with, we say that {\em optimizers} seek  ``candidate''(s)
  $x$ such that it is unlikely
 that there exists  ``better'' candidate(s) $y$. 
Each candidate  $c_i$ is a set of decisions and their
associated objective scores; i.e.  $c_i=(d,o)$. 
The optimizers in this article
assume  the existence of some fitness function $f$ that   converts {\em decisions} to
output objectives; i.e.
\[o = f(d)\]
Note that this paper uses the term ``model'' as a synonym for the fitness function $f$.
Also,
the terms single- and multi- objective
optimization apply when $\rvert o \rvert = 1$ and $\rvert o \rvert>1$, respectively.

Given two candidates $x,y$:
\bi
\item Each with objectives $x.o_i,y.o_i$ for
$1 \le i \le \rvert o \rvert $
\item Then 
$ x.o_i \prec y.o_i$ and 
$ x.o_i \succ y.o_i$ 
is true if   objective  $x.o_i$ is (worse,better) than $y.o_i$, respectively.
\ei
For single goal-optimization, the predicates
($\prec,\succ$) suffice to test is one candidate is ``better'' than another
For multi-objective optimization, determining ``better''  is somewhat more complicated 
Traditionally, the space of candidates with multiple
objectives was explored by assigning magic weights to the objectives, then using an {\em aggregation function} to accumulate the results. 
Such solutions may be  brittle; i.e. they change dramatically if we alter the magic weights of the objectives.  

To avoid the problem of magic weights, a multi-objective optimizer tries to produce the space of candidates that would be generated across all possible values of the magic weights.
Multi-objective evolutionary algorithms such as GALE, NSGA-II, SPEA2, IBEA, PSO, DE, MOEA/D, etc.~\cite{deb00afast,zit02,Zitzler04indicator-basedselection,Poli07particleswarm,585892,zhang07}, try to push a cloud of solutions towards an outer envelope of ``better'' candidates.
These algorithms eschew the idea of single solutions, preferring instead to use the {\em domination function} (discussed below) to map out the terrain of all useful candidates.

Generating a cloud of candidates may  be computationally
expensive. 
Suppose we divide 
the space of all functions  $f$ into subsets
$j \in f$,  $l \in f$. 
Let the  $j$ models be  very small and very  fast to execute.
The   evaluation cost $C$  of $j \in f$  is negligible
and we do not recommend GALE for these $C(j)\approx0$ models.

However for the larger models $l$, these many be 
prohibitively expensive to run
(particularly when
an optimizer must perform thousands to millions of evaluations).
Most optimizers (but not GALE) evaluate all $N$ candidates
and compare them $N^2$ times. GALE, on the other hand,
only evaluates and compares $2log_2(N)$ candidates. Hence,
we strongly
recommend  learners like GALE for these $C(l)>>0$ models.

When choosing an optimiser, it is useful
 to consider what information
an optimizer can access about the function $f$. 
For example, gradient descent optimizers~\cite{saltelli00} need to  access the contours around every decision.
This limits the kinds of functions they can process
to those with continuous differential functions (i.e. functions of real-valued variables whose derivative exists at each point in its domain).
Note that, when optimizing  software systems, these internal details may not
accessible. 
Ever since Parnas' 1972 paper ``On the Criteria To Be Used in Decomposing Systems into Modules''~\cite{parnas72}, software engineers have designed their systems as ``modules'' in which software's internal details are ``hidden'' within interface boundaries.  
This concept of modularity is one of the cornerstones of modern software engineering since, in such modular systems (1)~engineers are free to fix and enhance the internal details of software just as long as they maintain the same interface;
(2)~engineers can use the services of other systems by connecting to its interface {\em without} needing to understand the internal details.
We call this class
of  problems {\em black-box} or {\em functional} optimization.

Harman et al.~\cite{harman12abc} argue that when optimizing
software systems, a functional approach is very useful:
\begin{quote}
``{\ldots}the virtual nature of software makes it well suited for (search-based optimization). The field of SE is imbued with rich metrics that can be useful initial candidates for fitness functions{\ldots}(where) fitness is computed directly in terms of the engineering artifact, without the need for the simulation and modeling inherent in all {\em other approaches}.''
\end{quote}
By  ``other approaches'', Harman et al. refer to optimizers that demand detailed knowledge  about  the internals of a system
such as gradient descent optimizers.
Another example of this ``other approach'' was explored by
Sayyad, Menzies et al.~\cite{sayyad13a,sayyad13b}.
In that work,
multi-objective optimization was applied  systems containing hierarchical  constraints, where   the analysis could access all knowledge of  internal structure.
That structural knowledge was exploited via {\em push} and {\em pull} strategies that use decisions made in one part of a system to reduce the search space elsewhere in the system. 

In order to distinguish this ``other approach''
we call these  problems {\em white box} or {\em structural}
optimization. In our opinion, based
on a reading of the current literature, functional
optimization is more widely-applicable hence more widely-used than structural optimization. Hence,
this paper explores  functional optimizers like GALE since these will have more application areas.
  


  
\subsection{Search-based SE = MOEA + SE}\label{sec:abutsbse}\label{sec:w}

Evolutionary optimizers explore {\em populations} of candidate solutions.
In each {\em generation} some {\em mutator} makes changes to the current population. 
A {\em select} operator then picks the best mutants which are then {\em combined} in some manner to become generation $i+1$.
This century, there has been much new work on multi-objective evolutionary algorithms (MOEA) with 2 or 3 objectives (as well as many-objective optimization, with many more objectives).    

Recently, there has been much interest in applying MOEAs to many areas of software engineering including requirements engineering, test case planning, software process planning, etc. 
This {\em search-based software engineering} is a  rapidly expanding area of research and a full survey of that work is beyond the scope of this paper (for extensive notes on this topic, see~\cite{harman12abc,harman14}). 

 

\subsection{MOEA and Domination}\label{sec:cdom}

To explore the space of promising solutions, MOEA tools use a {\em  domination function} to find promising solutions for use in the next generation. 
Domination functions have the property that, when they compare candidate solutions with many competing objectives, they accept large sets (and not just single items) as being better than others.
Hence, they are candidate techniques for generating the space of possible solutions.

{\em Binary domination} says that solution $x$ ``dominates''
solution $y$ if solution $x$'s objectives are never worse than solution $y$ and at least one objective in solution $x$ is better than its counterpart in $y$; i.e.
 $\left\{ \forall o  \in \textit{objectives}\;\mid\; \neg ( x_o \prec y_o )\right\}$
and
 $\left\{
\exists o \in \text{objectives} \;\mid\; ( x_o \succ y_o ) \right\}$
where ($\prec,\succ$) tests if $x_o$ is (worse,better) than $y_o$.
Recently, Sayyad~\cite{sayyad13a} studied binary domination for MOEA with 2,3,4 or 5 objectives.  
Binary domination performed as well as anything else for 2-objective problems but very few good solutions were found for the 3,4,5-goal problems.  
The reason was simple: binary domination  only returns {\em \{true,false\}}, no matter the difference between $x_1,x_2$. 
As the objective space gets more divided at higher dimensionality, a more nuanced approach is required.


While binary domination just returns (true,false), a {\em continuous domination} function sums the total improvement of solution $x$ over all other solutions~\cite{Zitzler04indicator-basedselection}.
In the IBEA genetic algorithm~\cite{Zitzler04indicator-basedselection}, continuous domination is defined as the sum of the differences between objectives (here ``$o$'' denotes the number of objectives), raised to some exponential power.
Continuous domination favors $y$ over $x$ if $x$ ``losses'' least:

\begin{equation}\label{eq:cdom}
\begin{array}{rcl}
\textit{worse}(x,y)& =& \textit{loss}(x,y) > \textit{loss}(y,x)\\
\textit{loss}(x,y)& = &\sum_j^o -e^{w_j(x_j - y_j)/o	} / o
\end{array}
\end{equation}
In the above, $w_j\in \{-1,1\}$, depending on whether we seek to maximize goal $x_J$.   
To prevent issues with exponential functions, the objectives are normalized.

\lstset{
    language=Python,
    basicstyle=\ttfamily\fontsize{2.7mm}{0.8em}\selectfont,
    breaklines=true,
    prebreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=l,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    keywordstyle=\bfseries,
    emph={furthest,gale,better,improved,where,fastmap,split,project,mutate,mutate1}, emphstyle=\bfseries\color{blue},
    stringstyle=\color{green!50!black},
    commentstyle=\color{gray}\itshape,
    numbers=left,
    captionpos=t,
    escapeinside={\%*}{*)}
}

\subsection{MOEA Algorithms}\label{sec:algo}
A standard MOEA strategy is to generate new individuals, and then focus just on those on the Pareto frontier. 
For example, NSGA-II~\cite{deb00afast} uses a non-dominating sort procedure to divide the solutions into {\em bands} where {\em band}$_i$ dominates all of the solutions in {\em band}$_{j>i}$ (and NSGA-II favors the least-crowded solutions in the better bands).

 

There are other kinds of MOEA algorithms including the following (the following list is not exhaustive since, to say the least, this is a very active area of research):
\bi
\item
{\em SPEA2}: favors solutions that dominate the most number of other solutions that are not nearby (to break ties, it uses density sampling)~\cite{zit02}; 
\item {\em  IBEA}: uses continuous dominance to find the solutions that dominate all others~\cite{Zitzler04indicator-basedselection}; 
\item In {\em Particle swarm optimization}, a ``particle'''s velocity is 
``pulled" towards the individual and the community's best current solution~\cite{pan08,V.Sedenka2010,Kennedy:2001:SI:370449,Poli07particleswarm,Coello04,Reyes-sierra06};
\item 
The {\em many-objective optimizers}  designed for very high numbers of objectives~\cite{deb14}; 
\item
Multi-objective {\em differential evolution}: members of the frontier compete (and are possibly replaced) by candidates generated via extrapolation among  any three other members of the frontier~\cite{storn97,5601760,abbass01,robic05};
\item
The {\em decomposition methods} discussed below.
\ei 


\subsection{MOEA and Decomposition}\label{sec:decomp}

Another way to explore solutions is to apply some heuristic to decompose the total space into many smaller problems, and then use a simpler optimizer for each region. 
For example, in $\mathcal{E}$-domination~\cite{deb05}, each objective $o_i$ is divided into equal size boxes of size $\mathcal{E}_i$ (determined by asking users ``what is their lower threshold on the size of a useful effect?'').  
Each box has a set $X.\mathit{lower}$ containing boxes with worse $o_i$ values.  
Solutions in the same box are assessed and pruned in the usual way (all-pairs computation of a dominance function). 
But solutions in different boxes can be quickly pruned via computing dominance for small samples from each box. 
Once a box $X$ is marked ``dominated'', then $\mathcal{E}$-domination uses the boxes like a reverse index to quickly find all solutions in $X.\mathit{lower}$, then mark them as ``dominated''.

Later research generalized this approach. MOEA/D (multiobjective
evolutionary algorithm based on decomposition~\cite{zhang07}) is a generic framework that decomposes a multiobjective optimization problem into many smaller single problems, then applies a second optimizer to each smaller subproblem, simultaneously. 

GALE uses MOEA decomposition but avoids certain open issues with   $\mathcal{E}$-domination and MOEA/D. 
GALE does not need some outside oracle to specify $\mathcal{E}$. 
Rather, the size of the subproblems is determined via a recursive median split on dimensions synthesized using a PCA-approximation algorithm---see the  {\em fast spectral learning} described in the next section. 
Also, GALE does not need MOEA/D's secondary optimizer to handle the smaller subproblems. 
Rather, our approach uses  the synthesized dimensions to define the {\em geometry-based mutator} discussed below that ``nudges'' all candidates in a subproblem towards the better half of that subproblem. 

 



When domination is applied to a population it can be used to generate the {\em Pareto frontier}, i.e.  the space of non-dominated and, hence, most-preferred solutions.  
However, if applied without care, the number of evaluations of candidate solutions can accumulate.  
The goal of GALE is to minimize this number of evaluations, via applying the {\em fast spectral learning} and {\em active learning} techniques discussed in the next two sections.




%% \begin{figure}[!t]
%% {\fontsize{2mm}{0em}\selectfont 
%% ~~~~~~~~\begin{minipage}[t]{1.5in}
%% {\small 
%% {\bf Figure4a:} 
%% Dividing 400 instances using just the independent variables.}

%% \begin{verbatim}
%% 400
%% |  > 200
%% |  |  > 100
%% |  |  |  > 50
%% |  |  |  |  > 25
%% |  |  |  |  < 25
%% |  |  |  < 50
%% |  |  |  |  > 25
%% |  |  |  |  < 25
%% |  |  < 100
%% |  |  |  > 50
%% |  |  |  |  > 25
%% |  |  |  |  < 25
%% |  |  |  < 50
%% |  |  |  |  > 25
%% |  |  |  |  < 25
%% |  < 200
%% |  |  > 100
%% |  |  |  > 50
%% |  |  |  |  > 25
%% |  |  |  |  < 25
%% |  |  |  < 50
%% |  |  |  |  > 25
%% |  |  |  |  < 25
%% |  |  < 100
%% |  |  |  > 50
%% |  |  |  |  > 25
%% |  |  |  |  < 25
%% |  |  |  < 50
%% |  |  |  |  > 25
%% |  |  |  |  < 25
%% \end{verbatim}
%% \end{minipage}~~~~~~~~~\begin{minipage}[t]{1.5in}
%% {\small
%% {\bf Figure4b}: Dominated sub-trees are pruned via active learning techniques(~\tion{al}).}

%% \begin{verbatim}
%% 400           
%% |  > 200  <-- pruned
%% |  < 200 
%% |  |  > 100 
%% |  |  |  > 50 <-- pruned
%% |  |  |  < 50 
%% |  |  |  |  > 25 <-- pruned
%% |  |  |  |  < 25 
%% |  |  < 100 
%% |  |  |  > 50 
%% |  |  |  |  > 25 
%% |  |  |  |  < 25 
%% |  |  |  < 50 
%% |  |  |  |  > 25 
%% |  |  |  |  < 25 
%% \end{verbatim}
%% \end{minipage}}
%% \caption{Recursive division via WHERE of 400 instances 
%% (using FastMap).}\label{fig:cluster1}
%% \end{figure}
 


\subsection{Fast Spectral Learning}\label{sec:spectral}
This section describes how GALE decomposes a large space of candidate solutions into many smaller regions.

WHERE is a {\em spectral learner}~\cite{KamvarKM03}; i.e. given solutions with $d$ possible decisions, it re-expresses those $d$ decision variables in terms of the $e$ eigenvectors of that data.
This speeds up the reasoning since we then only need to explore the $e\ll d$   eigenvectors.

A widely-used spectral learner is a principal components analysis (PCA). For example, PDDP ({\em Principal Direction Divisive Partitioning})~\cite{boley98} recursively partitions data according to the median point of data projected onto the first PCA component of the current partition.

WHERE~\cite{me12d} is a linear time variant of PDDP  that uses FastMap~\cite{Faloutsos1995} to quickly find the first component.
Platt~\cite{platt05} shows that FastMap is a  Nystr\"om algorithm that finds approximations to eigenvectors.
As shown in \fig{fastmapCode} on lines 3,4,5, FastMap  projects all data onto a line connecting two distant points\footnote{
To define distance, WHERE uses the standard Euclidean distance method proposed by Aha et al.~\cite{aha91}; that is: $dist(x,y)= \sqrt{\sum_{i\in d} (x_i - y_i)^2}/\sqrt{ \left\vert{d}\right\vert }$ where distance is computed on the independent decisions $d$ of each candidate solution; all $d_i$ values are normalized min..max, 0..1; and the calculated distance normalized by dividing by the maximum distance across the $d$ decisions.}. 
FastMap finds these two distant points in near-linear time. 
The search for the poles needs only $O(N)$ distance comparisons (lines 19 to 24).
The slowest part of this search is the sort used to find the median $x$ value (line 10) but even that can be reduced to  asymptotically optimal linear-time via the standard median-selection algorithm~\cite{hoare61}.

FastMap returns the data split into two equal halves.
WHERE recurses on the two halves, terminating when some split has less than $\sqrt{N}$ items.


\begin{figure}[!t] 
\begin{minipage}{3.2in}
\begin{lstlisting}[mathescape,frame=r,numbers=right]
def fastmap(data): 
  "Project data on a line to 2 distant points"
  z          = random.choose(data)
  east       = furthest(z, data)
  west       = furthest(east, data)
  data.poles = (west,east)
  c          = dist(west,east)     
  for one in data.members: 
    one.pos = project(west,east,c,one)
  data = sorted(data) # sorted by 'pos'
  return split(data)

def project(west, east, c, x): 
  "Project x onto line east to west"
  a = dist(x,west)
  b = dist(x,east)
  return (a*a + c*c - b*b)/(2*c) # cosine rule

def furthest(x,data): # what is furthest from x?
  out, max = x,0
  for y in data:
    d = dist(x,y)
    if d > max: out, max = y, d
  return out

def split(data): # Split at median
   mid = len(data)/2; 
  return data[mid:], data[:mid]
\end{lstlisting}
\caption{Splitting data with FastMap}
\label{fig:fastmapCode}  
\end{minipage}
\end{figure}

\subsection{Active Learning}

One innovation in GALE is its use of  {\em active learning} during WHERE's decomposition of larger problems into sub-problems.
Active learners make conclusions by asking for {\em more} information on the {\em least} number of items.  
For optimization,  such active learners reflect over a population of decisions and only compute the objective scores for a small, {\em most informative subset} of that population~\cite{Zuluaga:13}. 
GALE's active learner finds its {\em most information subset} via the WHERE clustering procedure described above.
Recall that
WHERE recursively divides the candidates into many small clusters, and then looks for two most different (i.e. most distant) points in each cluster. 
For each cluster, GALE then evaluates only these two points. 

In other work, Zuluaga et al.~\cite{Zuluaga:13} use a {\em response surface method} for their MOEA active learner.  
Using some quickly-gathered information, they build an approximation to the local Pareto frontier using a set of Gaussian surface models. 
These models allow for an extrapolation from  known members of the
population to new and novel members.  
Using these models, they can then generate approximations to the objective scores of mutants. 
Note that this approach means that (say) after 100 evaluations, it becomes possible to quickly approximate the results of (say) 1000 more.


\begin{figure}[!b]
\begin{minipage}{3.2in}
\hspace{0.4cm}\begin{lstlisting}[mathescape,frame=l,numbers=left]
def where(data, scores={},lvl=10000,prune=True):  
  "Recursively split data into 2 equal sizes."
  if lvl < 1: 
     return data # stop if out of levels
  leafs      = [] # Empty Set
  left,right = fastmap(data)
  west, east = data.poles
  $\omega=\sqrt{\mu}$ # enough data for recursion
  goWest = len(left) > $\omega$  
  goEast = len(right) > $\omega$ 
  if prune: # if not pruning, ignore this step
    if goEast and better(west,east,scores): 
       goEast = False 
    if goWest and better(east,west,scores): 
       goWest = False 
  if goWest:  
     leafs += where(left,  lvl - 1, prune)  
  if goEast:  
     leafs += where(right, lvl - 1, prune) 
  return leafs

def better(x,y,scores):
   "Check  not worse(y,x) using $\eq{cdom}$. If"
   "any new evaluations, cache   in 'scores'."
\end{lstlisting}

\caption{Active learning in GALE:
recursive division of the data;
only evaluate two distant points in each cluster;
only recurse into non-dominated halves.
In this code, $\mu$ is the size of the original data set.
}
\label{fig:whereCode} 
\end{minipage}
\end{figure}


 
Unlike Zuluaga et al., GALE makes no Gaussian parametric assumption about regions on the Pareto frontier.  
Rather, it uses a non-parametric approach (see below).
That said, GALE and  Zuluaga et al.  do share one  assumption; i.e that the Pareto frontier can be approximated by many tiny models. 
 

\subsection{Preference-Based MOEA}

GALE's active learner can be viewed as a tool that biases a search towards ``interesting''
regions in the search space. The results shown below indicate that this kind of biasing
can find solutions much faster than, say, the standard random employed by genetic algorithms.

The potential weakness of random
mutation has been recognised by the evolutionary computing community for a long time.
Various improvements on random search have been proposed.
For example, Peng et al.~\cite{peng09:ls} have augmented MOEAs with
local search  (i.e. applying a problem-specific repair/improvement
heuristic on some current solution).
Also, Igel et al.'s~\cite{igel07} multi-objective
covariance matrix adaptation evolution strategy
can run the mutations along  ``ridges'' in the search space.
However, prior to this paper, no such work has appeared
in   SE . Also, to the best of our knowledge, GALE's cost reduction of MOEA
to $O(2log_2N)$ evaluations  has not been previously reported in the SBSE literature.





\section{Inside GALE}

As a summary, the  {\em \underline{g}eometric}, {\em \underline{a}ctive \underline{le}arner} called GALE works as follows:
 
\be
\item Sort solutions (along the direction of most change);
\item Find the {\em poles}; i.e. the two most distance candidates;
\item Split that sort into equal halves;
\item Evaluate only the {\em poles} of each split;
\item Ignore any half containing a dominated pole;
\item Recurse on the remaining halves until the splits get too small (less than $\sqrt{N}$);
\item For all the final (smallest) splits,
  mutate the candidates towards the better pole of
  that split. 
\item Go to step \#1 \ee 
 
\noindent

Because, at each point, GALE makes a linear approximation, a naive assumption might be that GALE can only solve linear problems.
This is untrue.
GALE recursively bisects the solutions into progressively smaller regions using the spectral learning methods discussed in \tion{spectral}. Spectral learners reflect over the eigenvectors of the data.
These vectors are a model of the overall direction of the data.  
Hence, GALE's splits are not some naive division based on the raw dimensions. 
Rather, GALE's splits are very informed about the overall shape of the data. 
GALE's recursive splitting generates a set of tiny clusters. Each cluster represents a small space on the Pareto frontier.  
That is, GALE does not assume that the whole Pareto frontier can be modeled as one straight line. 
Rather, it assumes that the Pareto frontier can be approximated by a set of very small {\em locally linear} models.

GALE interfaces to models using the following functions:
\bi
\item Models create candidates, each  with $d$ decisions. 
\item $\mathit{lo}(i), \mathit{hi}(i)$ report the minimum and maximum
legal values for decision $i\in d$.
\item $\mathit{valid}(\mathit{candidate})$ 
checks if the decisions
do not violate any domain-specific constraints.
\item  From the decisions,
a model can compute $o$  objective scores (used in \eq{cdom}).
\item $\mathit{minimizing}(j)$ returns true,false if the goal
is to minimize,maximize (respectively) objective $j \in o$.
\ei
We discuss these functions further in the following sections.

%% GALE explores the candidates via a combination of optimization and data
%% mining. Given a current population of candidates, the
%% algorithm learns a decision tree whose leaves are non-dominated
%% candidates. These leaves are the seeds for the next generation 
%% of candidates.




\subsection{Active Learning and GALE}\label{sec:al}
GALE's active learner, shown in \fig{whereCode}, is a variant to the WHERE spectral learner discussed above.
To understand this procedure, recall that WHERE splits the data into smaller clusters, each of which is characterized by two distant points called {\em west,east}. 
In that space, {\em left} and {\em right} are  50\% of the data, projected onto a line running {\em west} to {\em east}, split at the median.
When exploring $\mu$ candidates, recursion halts at splits smaller than $\omega=\sqrt{\mu}$.

GALE's active learner assumes that it only needs to evaluate
the {\em most informative subset} consisting of the {\em poles} used to
recursively divide the data. 
Using \eq{cdom}, GALE checks for domination between the poles and only recurses into any non-dominated halves.
This process, shown in \fig{whereCode}, uses FastMap to split the data. 
In  \fig{whereCode}, lines 12 and 14 show the domination pruning that disables recursion into any dominated half.


Given GALE's recursive binary division of $\mu$ solutions, and that this domination tests only two solutions in each division, then GALE performs  a maximum of $2log_2(\mu)$ evaluations. 
Note that when GALE prunes sub-trees, the actual number of evaluations is less than this maximum.


\subsection{Geometry-based Mutation}\label{sec:geom}
Most MOEAs build their next generation of solutions by a {\em random mutation} of members of the last generation. 
GALE's mutation policy is somewhat different in that it is a {\em directed mutation}.
Specifically, GALE reflects on the geometry of the solution space, and mutates instances along gradients within that geometry. 

To inspect that geometry, GALE reflects over the poles in each leaf cluster. 
When one pole is {\em better} than another, it makes sense to nudge all solutions in that cluster away from the worse pole and towards the better pole.
By nudging solutions along a line running from {\em west} to {\em east}, we are exploiting spectral learning to implement a {\em spectral mutator}; i.e. one that works across a dimension of greatest variance that is synthesized from the raw dimensions.  
That is, GALE models the local Pareto frontier as many linear models
drawn from the local eigenvectors of different
regions of the solution space. 
 
GALE's mutator is shown in \fig{mutantCode}.
The $\Delta$ parameter is the ``accelerator'' that increases mutation size (in line 20) while the  $\gamma$ parameter is the ``brake'' that blocks excessive mutation (in line 24).

 

\subsection{Top-level Control}

\fig{galeCode} shows GALE's top-level controller.  
As seen in that figure, the algorithm  is an evolutionary learner which iteratively builds, mutates, and prunes a population of size $\mu$ using the active learning version of WHERE.
The {\em candidates} function (at line 3 and 18) adds random items to the population. 
The first call to this function (at line 3) adds $\mu$ new items. The subsequent call (at line 18) rebuilds the population back up to $\mu$ after WHERE has pruned solutions in dominated clusters.

Also shown in that figure is GALE's termination procedure: GALE exits after $\lambda$ generations with no improvement in any goal.
Note that, on termination, GALE calls WHERE one last time at line 15 to find {\em enough} examples to show the user. 
In this call, domination pruning is disabled, so this call returns the poles of the leaf clusters.


\begin{figure}[!t]
\begin{minipage}{3.2in}
\begin{lstlisting}[mathescape,frame=r,numbers=right]
def mutate(leafs, scores): 
  "Mutate all candidates in all leafs."
  out = [] # Empty Set
  for leaf in leafs:
    west,east = leafs.poles
    if better(west,east, scores): # Equation 1
       east,west = west,east # east =  best pole
    c = dist(east,west)
    for candidate in leaf.members:
      out += [mutate1(candidate, c, east, west)]
  return out 

def mutate1(old,c,east,west,$\gamma$=1.5,$\Delta$=1): 
  "Nudge the old towards east, but not too far."
  new    = copy(old)
  for i in range(len(old)):
    d = east[i] - west[i]
    if not d == 0: #there is a gap east to west
      d= -1 if d < 0 else 1 #d is the  direction
      x= $\Delta$*new[i]* (1+ abs(c)*d) #nudge along d 
      new[i]= max(min(hi(i),x),lo(i)) #trim 
  newDist = project(west,east,c,new) -
            project(west,east,c,west)
  if (abs(newDist) < $\gamma$*abs(c)) and valid(new): 
    return new
  else: return old
\end{lstlisting}

\caption{Mutation with GALE.
By line 7, GALE has determined that the {\em east}
pole is preferred  to {\em west}.
At line 23,24, the {\tt project}
function of \fig{fastmapCode}  is used
to check 
we are not rashly mutating a candidate too far away from
the region that originally contained it.}
\label{fig:mutantCode} 
\end{minipage}
\end{figure}
\begin{figure}[!t]
\begin{minipage}{3.2in}
\begin{lstlisting}[mathescape,frame=r,numbers=right]
def gale(enough=16,  max=1000,  $\lambda=3$):
  "Optimization via Geometric active learning"
  pop = candidates($\mu$) # the initial population
  patience = $\lambda$
  for generation in range(max):
    # mutates candidates in non-dominated leafs
    scores  = {} #cache for  objective scores 
    leafs   = where(pop, scores)
    mutants = mutate(leafs, scores)
    if generation > 0:  
      if not improved(oldScores ,scores):
        patience = patience - 1 #less  patience
      oldScores = scores #need in next gen
      if patience < 0: #return enough candidates
       leafs=where(pop,{},log2(enough),prune=no)
       return [ y.poles for y in leafs ] 
    #build up pop for next generation
    pop = mutants + candidates($\mu$-len(mutants))   

def improved(old,new):
  "Report some success if any improvement."
  for j in range(len(old)):
    before = # old mean of the j-th objective
    now    = # new mean of the j-th objective
    if minimizing(j):
      if now < before: return True
    elif now > before: return True
  return False 
\end{lstlisting}

\caption{GALE's top-level driver.}
\label{fig:galeCode}
\end{minipage}
\end{figure}

\input{cocont}


 

\section{Models Used in This Study}\label{sec:models}
Having described general details on MOEA, and the particular details of our approach, we turn now to the models used to evaluate GALE.
With one exception, all these  are available to other researchers via the websites mentioned in \tion{avail}. 

The exception is the CDA model since that requires extensive connection to proprietary NASA hardware and software. One important feature of
CDA is that it takes hours to complete
a single evaluation. Hence, it is an good example for exploring
the advantages of  GALE's active learning.


\input{xomo}

\input{xomocases}
 

\begin{figure*}%[!t]
\scriptsize

  \centering
    \begin{tabular}{|l|l|p{4in}|c|}
        \hline
        Short name &Decision             & Description         &Controllable                                        \\ \hline
        Cult&Culture              & Number (\%) of requirements that change. & yes \\\hline
        Crit&Criticality           & Requirements cost effect for safety critical systems  (see \eq{cmcrit}). & yes\\\hline
        Crit.Mod&Criticality Modifier & Number of (\%) teams affected by criticality   (see \eq{cmcrit}).   & yes           \\ \hline
        Init. Kn&Initial Known        & Number of (\%) initially known requirements.             & no     \\ \hline
        Inter-D&Inter-Dependency     & Number of (\%) requirements that have interdependencies.  Note that dependencies are requirements within
the {\em same} tree (of requirements), but interdependencies are requirements that live in {\em different} trees.   & no            \\\hline
        Dyna&Dynamism             & Rate of how often new requirements are made (see \eq{dymn}). & yes                    \\ \hline
        Size&Size            & Number of base requirements in the project.& no \\        \hline
        Plan&Plan                 & Prioritization Strategy (of requirements): one of
        0= Cost Ascending;  1= Cost Descending; 2= Value Ascending; 3= Value Descending;
        4 = $\frac{Cost}{Value}$ Ascending.
%Note that a standard agile strategies use ``Value Descending'', i.e. plan=3~\cite{me09j}.
 & yes \\\hline
     T.Size&Team Size            & Number of personnel in each team   & yes                         \\ 
        \hline
    \end{tabular}
    \caption {List of Decisions used in POM3. 
The optimization task is to find settings for the controllables in the last column.
}\label{fig:pom3decisions}
\end{figure*}
\begin{figure*}%[!t]
\footnotesize
\begin{center}
    \begin{tabular}{r|p{1.5in}|p{1.5in}|p{1.5in}}
                     & POM3a                         & POM3b             &POM3c       \\ 
                             & A broad space of projects. & Highly critical small projects& Highly dynamic large projects\\\hline
        Culture              & 0.10 $\leq x \leq$ 0.90       & 0.10 $\leq x \leq$ 0.90  & 0.50 $\leq x \leq$ 0.90  \\ 
        Criticality          & 0.82 $\leq x \leq$ 1.26       & 0.82 $\leq x \leq$ 1.26   & 0.82 $\leq x \leq$ 1.26  \\ 
        Criticality Modifier & 0.02 $\leq x \leq$ 0.10       & 0.80 $\leq x \leq$ 0.95 & 0.02 $\leq x \leq$ 0.08   \\ 
        Initial Known        & 0.40 $\leq x \leq$ 0.70       & 0.40 $\leq x \leq$ 0.70  & 0.20 $\leq x \leq$ 0.50  \\ 
        Inter-Dependency     & 0.0   $\leq x \leq$ 1.0       & 0.0   $\leq x \leq$ 1.0  & 0.0   $\leq x \leq$ 50.0 \\ 
        Dynamism             & 1.0   $\leq x \leq$ 50.0      & 1.0   $\leq x \leq$ 50.0  & 40.0   $\leq x \leq$ 50.0 \\ 
        Size                 & x $\in$ [3,10,30,100,300] & x $\in$ [3, 10, 30]     & x $\in$ [30, 100, 300]   \\ 
        Team Size            & 1.0 $\leq x \leq$ 44.0        & 1.0 $\leq x \leq$ 44.0  & 20.0 $\leq x \leq$ 44.0    \\ 
        Plan                 & 0 $\leq x \leq$ 4             & 0 $\leq x \leq$ 4    & 0 $\leq x \leq$ 4       
\end{tabular}
\end{center}

\caption{Three classes of projects studied using POM3. }\label{fig:POM3abcd}
\end{figure*}




\subsection{POM3: A Model of Agile Development}\label{sec:pom3pom3} 
According to Turner and Boehm,  the agile management challenge is to strike a balance between {\em idle rates}, {\em completion rates} and {\em overall cost}. 
\bi
\item
In the agile world, projects terminate after achieving a {\em completion rate} of   $(X<100)$\% of its required tasks.
\item
Team members  become {\em idle} if forced to wait for a yet-to-be-finished task from other teams. 
\item
To lower {\em idle rate} and increase {\em completion rate}, management can hire staff--but this increases  {\em overall cost}.
\ei 


The POM3 model~\cite{port08,1204376}  is a tool
for exploring that management challenge.
POM3 implements the Boehm and Turner model of agile programming~\cite{turner03} where teams select tasks as they appear in the scrum backlog. POM3 can  studu the implications of different ways to adjust task lists in the face of shifting priorities.


 
In this study, our optimizers tune the 
POM3  decisions of \fig{pom3decisions} in order to
\bi
\item Increase completion rates;
\item Reduce idle rates;
\item Reduce overall cost.
\ei 
For further details on this model see~\cite{port08,1204376,turner03}. A summary
of that model is shown below.

 POM3 represents requirements as a set of trees.  
    Each tree of the requirements
    heap represents a group of requirements wherein a single node of the
    tree represents a single requirement.  A single requirement consists
    of a prioritization value and a cost, along with a list of
    child-requirements and dependencies.  Before any requirement can be
    satisfied, its children and dependencies must first be satisfied.
    
   POM3 builds a requirements heap with prioritization values,
    containing
     30 to 500 requirements, with costs from 1 to 100 (values 
     chosen in consultation with Richard Turner).  Initially,
    some percent of the requirements are marked
    as visible, leaving the rest to be revealed as teams work on the
    project.

The task of completing a project's requirements is divided amongst teams
    relative to the size of the team (by ``size'' of team, we refer to
    the number of personnel in the team).  In POM3, team size is a decision
    input and is kept constant throughout the simulation.  As a further
    point of detail, the personnel within a team fall into one of three
    categories of programmers: Alpha, Beta and Gamma.  Alpha programmers
    are generally the best, most-paid type of programmers while Gamma
    Programmers are the least experienced, least-paid.  The ratio of personnel type
    follows the Personnel decision as set out by Boehm and Turner\cite{1204376} in the following table:    
    \[\scriptsize
    \begin{array}{l|l|l|l|l|l}
             \multicolumn{1}{c}{~}   & \multicolumn{5}{c}{\textit{project size}}\\\cline{2-6}
                & 0    & 1    & 2    & 3    & 4    \\ \hline
            \textit{Alpha} & 45\% & 50\% & 55\% & 60\% & 65\% \\ 
            \textit{Beta}  & 40\% & 30\% & 20\% & 10\% & 0\%  \\ 
            \textit{Gamma} & 15\% & 20\% & 25\% & 30\% & 35\% \\ 
        \end{array}
    \]
    After teams are generated and assigned to requirements, costs are
    further updated according to decision for the Criticality and
    Criticality Modifier.  Criticality affects the
    cost-affecting nature of the project being safety-critical, while the
    criticality modifier indicates a percentage of teams affected by
    safety-critical requirements.  In the formula, $C_M$ is the {\em criticality
    modifier}:
    \begin{equation}\label{eq:cmcrit}
    \textit{cost} = \textit{cost} * {C_M}^{\textit{criticality}}
    \end{equation}
    
After generating the Requirements \& Teams, POM3 runs through the follow five-part {\em shuffling} process (repeated   \mbox{$1 \le N \le 6$} times, selected at random).
    
    {\em 1) Collect Available Requirements.} Each team searches through
    their assigned requirements to find the available, visible
    requirements (i.e. those without any  unsatisfied dependencies
    or unsatisfied child requirements).  At this time, the team budget
    is updated, by calculating the total cost of tasks remaining for the
    team and dividing by the number of shuffling iterations:
    \[\textit{team.budget} = \textit{team.budget} + \textit{totalCost/numShuffles}\]
    
    {\em 2) Apply a Requirements Prioritization Strategy.}  After the
    available requirements are collected, they are then sorted per some
    sorting strategy. In this manner, requirements with higher priority
    are to be satisfied first. To implement this, the requirement's cost
    and value are considered along with a strategy, determined by the
    plan decision.
    
    {\em 3) Execute Available Requirements.} The team executes the
    available requirements in order of step2's prioritization.
    Note that some requirements may not get executed due to budget allocations.
    
    {\em 4) Discover New Requirements.} As projects mature, sometimes new requirements are discovered.  To model the probability of new requirement arrivals, the input decision called Dynamism is used in a Poisson distribution.  
    The following formula  is used to add to the percentage of known requirements in the heap:
    \begin{equation}\label{eq:dymn}
    \textit{new} = \textit{Poisson}\left(\textit{dynamism}/10\right)
    \end{equation}
    {\em 5) Adjust Priorities.} In this step, teams adjust their priorities by making use of the Culture $C$ and Dynamism 
    $D$ decisions.  Requirement values are adjusted per the formula along a normal distribution, and scaled by a project’s culture:
    \[
    \textit{value} = \textit{value} + \textit{maxRequirementValue}*\textit{Normal}(0, D)*C
    \]  

When we ran POM3 through various MOEAs, we noticed a strange pattern in the results
(discussed below). To check if that pattern was a function of the model or the MOEAs,
we ran POM3 for the three different kinds of projects shown in \fig{POM3abcd}.
We make no claim that these three classes represent the space of all possible projects.
Rather, we just say that for several kinds of agile projects,
GALE appears to out-perform NSGA-II and SPEA2.

%XXX flip XOMO and POM. make sure pom3abc is mentioned


\subsection{CDA: An Aviation Safety Model}\label{sec:cda}

The CDA model~\cite{Kim2011,Pritchett2011,Feigh2012,Kim2013,Pritchett2013,krall15:hms} lets an engineer explore the implications of how software presents an airplane's status to a pilot in safety critical situations. 
CDA models how pilots interact with cockpit avionics software during a continuous descent approach.
Internally, CDA models the physical aerodynamics of an aircraft's flight, the surrounding environment (e.g. winds), and the cognitive models and workload of the pilots, controllers and computers.

For this study, our optimizers tune the following decision variables:
\bi
\item {\bf HTM}: maximum human task load. This value describes how many tasks (where a task is an atomic action) can be maintained in a mental to-do list by a person.  When the number of necessary tasks exceeds the number of tasks that the person can maintain, there can be incurred delays, errors, or the possibility of the task being forgotten and lost.
\item {\bf FA:} function allocation. This variable refers mainly to the relative authority between the human pilot and the avionics.
\item {\bf CCM:} contextual control mode of pilots. These describe the pilots' ability to apply patterns of activity in response to the demands and resources in the environment.
\item {\bf SC:}
The air environment scenario. WMC’s CDA model includes four different arrival and approach scenarios.
\ei
These decisions were tuned in order to {\em reduce} all the following objectives:
\be
\item  
\emph{NumForgottenActions}:
tasks forgotten by the pilot;
\item 
{\em NumDelayedActions}: number of delayed actions;
\item
 \emph{NumInterruptedActions}: 
interrupted actions;
\item 
\emph{DelayedTime}:
total time of all of the delays;
\item 
{\em InterruptedTime}: time for
dealing with interruptions.
\ee
This paper   uses CDA as a large and complex model
to comparatively evaluate  GALE vs NSGA-II vs SPEA2.
Elsewhere~\cite{krall15:hms}, we offer an extensive discussion of the theory behind the CDA model
and the cognitive implications of the decisions made by GALE. 

\subsection{Benchmark Models}

Apart from the above three models, we also explore numerous small benchmark models that are often used to assess MOEA problems 
These models are called BNH, Golinski, Srinivas, Two-bar Truss, Viennet2, Water, ZDT(1,2,3,4 and 6), and DTLZ(1,2,3,4,5 and 6).
The  DTLZ suite of models is particularly useful for evaluation of MOEAs since, by adjusting certain model  parameters, it is possible to generate problems with a wide range of decisions and objectives~\cite{Zitzler2000zdtpaper,dtlz2001a}.

For full  details on the benchmark models, including their decisions
and objectives, see the the appendix
(available in the online supplemental material). 


% The model of primary interest in this paper is a case study on agile software
% development.  To provide a wide application of SBSE to this model, four scenarios
% of the POM3 model are defined as MOPs for JMOO.  These four scenarios were arbitrarily chosen as
% subsets of possible real-world software projects.

% \bi
% \item POM3a: a broad space of software projects.
% \item POM3b: small systems; highly critical tasks.
% \item POM3c: large systems; very dynamic environment.
% \item POM3d: small systems; very static environment.
% \ei

\section{Experimental Methods}

This section describes how we applied and compared various optimization algorithms using the models described above

\subsection{Comparison Optimization Algorithms}\label{sec:compares}
 
To  assess a new MOEA algorithm, the performance of the new algorithm needs
to be compared to existing approaches.
One important criteria for selecting those existing approaches is
 {\em repeatability}. 
Many of the algorithms described above such as MOEA/D and PSO
are really {\em frameworks} within which an engineer has free reign to make numerous decisions
(hence, 
review papers list dozens of variants on  PSO and MOEA/D ~\cite{5601760,Poli07particleswarm}).
Hence, in terms of {\em repeatability}, it can be better to use precisely defined algorithms like NSGA-II and SPEA2 rather than framework algorithms such as PSO and MOEA/D.

New MOEA  algorithms are being invented all the time.
Late in the development of this project, the authors became
aware of a new version of NSGA-II which, according to its
authors~\cite{6600851}, performed better for large number of objectives.
The merits of this new approach are still be assessed and some
results suggest that, in terms of improving objectives, it is not necessary a superior approach~\cite{sato14}. That said, we know of no similar work
in the SBSE literature that claims anything like GALE's large-scale
reductions in the number
of evaluations.

 

Comparison algorithms should also be {\em appropriate to task}.  
For example, Sayyad, Menzies et al.'s {\em push,pull} IBEA
extensions~\cite{sayyad13a,sayyad13b} were designed
for a very specialized problem (systems of
hierarchical constraints in which the optimizer has
total knowledge of all constraints within a model).
GALE, on the other hand, was designed for the more
general ``black-box'' SBSE problems described in
\tion{abutsbse} (no access to internal structure;
controllables are just a flat vector of model
inputs; a need to find solutions after a minimal
number of evaluations).

Yet another criteria is {\em accepted practice}. We reached out to our SBSE colleagues to
 find
which algorithms are accepted as ``best''.  However, no consensus was found.

Finally, we sought  what algorithms are {\em commonly used}.
In 2013, Sayyad and Ammar~\cite{sayyad13c} surveyed 36 SBSE papers where $\frac{21}{36}$
used NSGA-II or SPEA2 (of the others, 4 used some home-brew genetic algorithm and the remainder
each used some MOEA not used by any other paper). Since NSGA-II and SPEA2 
also score well on {\em repeatability},
they are used in the following evaluation.
 

\subsection{Implementations and Parameter Settings}\label{sec:expexp}
To provide a reusable experimental framework, we
implemented GALE as part of a Python software
package called JMOO (Joe's Multi-Objective
Optimization).  JMOO allows for
testing experiments with different MOEAs and
different MOPs (multi-objective problems), and
provides an easy environment for the addition of
other MOEAs.  
JMOO uses the DEAP
toolkit~\cite{jmlr12} for its
implementations of  NSGA-II and SPEA2.
NSGA-II and SPEA2 require certain
 parameters for crossover and
mutation. We used the defaults from  DEAP:
\bi
\item A crossover frequency of  $cx=0.9$;
\item The mutation rate is $mx=0.1$, and $eta=1.0$ determines how often
mutation occurs and how similar mutants are to their
parents (higher $eta$ means more similar to the
parent).
\ei

To provide a valid base of comparison, with the exception of the DTLZ models,
we applied nearly
the same parameter choices across all
experiments:
\bi
\item
MOEAs use the same
population$_0$ of size $\mu=100$.
\item All MOEAs had the same early stop criteria (see the
$\lambda=3$ test of \fig{galeCode}).
Without early stop, 
number of generations is set at {\em max}$=20$.
\item
\fig{mutantCode}'s mutators used \mbox{$\Delta=1$}, \mbox{$\gamma=1.5$}.
\ei
 
There was one case where we adjusted these defaults. 
DTLZ are artificial models designed to test certain hard optimization problems.
The shape of the DTLZ Pareto frontiers are somewhat unusual:
their
objective scores change slowly
across a smooth surface (whereas the frontier of
many other models we have examined have more jagged hills and valleys
in any local region). Accordingly, for DTLZ, we increased the
$\Delta$ ``accelerator'' parameter on GALE's mutator
(discussed in \tion{geom}) from $\Delta=1$ to $\Delta=3$
so that  GALE's search for better solutions
``jumped'' further across the DTLZ frontiers.

One final detail: to ensure an ``apples vs. apples'' comparison, 
each of our optimizers was run on the same randomly generated
initial population for each problem. That is, all optimizers had the same starting point.
 


 
\subsection{Evaluation Criteria}\label{sec:eval}
An ideal optimizer explores a  large {\em
  hypervolume} of solutions;  offers many  ``best''
solutions that are very {\em spread} out on the outer
frontier of that volume; 
offers most {\em improvement} to objective scores; and does all this
using fewest evaluations (the last item is  important when the model is slow to evaluate, or
when humans have to audit the conclusions by reviewing the
optimizer's decisions).

For these evaluation criteria:
\bi
\item
{\em Larger} values are {\em better}  for {\em hypervolume};
\item 
{\em Smaller} values are {\em better}  for number of {\em evaluations} and {\em spread} and {\em improvement} to objective scores.
\ei
To explain why {\em smaller} values for {\em spread} and {\em improvement} are {\em better},
we offer the following notes:
\bi
\item
Deb's {\em spread} calculator~\cite{deb00afast} includes
the term 
$\sum_i^{N-1} (d_i - \overline{d})$ 
where $d_i$
is the distance between adjacent solutions and
$\overline{d}$ is the mean of all such values.  A
``good'' spread makes all the distances equal ($d_i
\approx \overline{d}$), in which case Deb's spread
measure would reduce to some minimum value.
\item
As to {\em improvement}, we measure this quality using the {\em loss} calculation
of Equation~1 
by  comparing mean values of objective scores from instances in (1)~a baseline population prior to optimization
to (2)~the population of the final frontier after optimization terminates.
Here, 
less loss is better so smaller values for {\em improvement} are desirable. 
\ei
Finally, for some models, we offer visualizations of the raw objective scores, and how they change
as the number of evaluations change. As seen below, sometimes these ``raw'' visualizations
offer insights that can be missed by summary statistics such as hypervolume, spread, and improvement. 
 



%% \begin{figure}[!b]
%% \color{MyDarkBlue}
%% \begin{center}\scriptsize
%%     \begin{tabular}{|r|c|c|c|c|r|c|} \hline
%%     	Model		&		\begin{sideways}\#~decisions\end{sideways}	&		\begin{sideways}\#~objectives\end{sideways}	&	Type			&	\begin{sideways}\#~constraints\end{sideways}	        		&	Runtime   & Reference\\ \hline \hline
%%     	CDA			&   	4	&   	5   	& Large		&   	0           		&    8100  &   \cite{Kim2011,Pritchett2011,Feigh2012,Kim2013,Pritchett2013} \\ \hline
%% 	POM3C		&	9	&	4	&   Large		&	0			&	94.798 & \cite{port08,me09j}\\  \hline
%% 	POM3A		&	9	&	4	&   Large		&	0			&	47.465 	& \cite{port08,me09j}\\  \hline
%%     	POM3B		&	9	&	4	&   Large		&	0			&	 2.060 &  \cite{port08,me09j} \\  \hline
%%     	Tanaka		&	2	&	2	& 	Constrained &   2           			&    0.326   & \cite{537993}  \\ \hline
%%     	Osyczka2	&	6	&	2	&   Constrained &   6           				&    0.250  & \cite{osymodel}   \\ \hline
%%     	ZDT3		&	30	&	2	& Unconstrained &   0           			&    0.042   & \cite{Zitzler2000zdtpaper}   \\ \hline
%%     	ZDT2		&	30	&	2	& Unconstrained &   0           			&    0.039   & \cite{Zitzler2000zdtpaper}   \\ \hline
%%     	ZDT1		&	30	&	2	& Unconstrained &   0           &    0.038   & \cite{Zitzler2000zdtpaper}  \\ \hline
%%     	ZDT4		&	10	&	2	& Unconstrained &   0           &    0.028   & \cite{Zitzler2000zdtpaper}   \\ \hline
%%     	Srinivas	&	2	&	2	&	Constrained &   2           &    0.024   & \cite{DBLP:journals/ec/SrinivasD94}  \\ \hline
%%     	ZDT6		&	10	&	2	& Unconstrained &   0           &    0.020   & \cite{Zitzler2000zdtpaper}   \\ \hline
%%     	Golinski	& 	7	&	2	& Unconstrained &   0           &    0.014  & \cite{golinskimodel}   \\ \hline
%%     	Kursawe     &   3   &   2   & Unconstrained &   0           &    0.013   & \cite{Kursawe91avariant}  \\ \hline
%%     	Fonseca     &   3   &   2   & Unconstrained &   0           &    0.012    & \cite{Fonseca95anoverview} \\ \hline
%% 	Poloni      &   2   &   2   & Unconstrained &   0           &    0.012  & \cite{polonimodel}   \\ \hline
%% 	Constrex    &   2   &   2   &   Constrained &   2           &    0.011    & \cite{deb00afast}   \\ \hline	
%%     	TwoBarTruss &   3   &   2   &   Constrained &   1           &    0.011   & \cite{Chafekar03constrainedmultiobjective}   \\ \hline	
%%     	Water       &   3   &   5   &   Constrained &   5           &    0.011  & \cite{watermodel}   \\ \hline
%%     	Viennet3	&	2	&	3	& Unconstrained &   0           &    0.010  & \cite{viennetmodels}   \\ \hline
%%     	Viennet4	&	2	&	3	& Unconstrained &   0           &    0.009   & \cite{viennetmodels}  \\ \hline	
%% 	BNH         &   2   &   2   &   Constrained &   2           &    0.008   & \cite{Binh97mobes:a}		\\ \hline
%%     	Viennet2	&	2	&	3	& Unconstrained &   0           &    0.008 & \cite{viennetmodels}    \\ \hline
%%     	Schaffer	&	1	&	2	& Unconstrained &   0           &    0.006   & \cite{Schaffermodel}   \\ \hline    	
%%     \end{tabular}
%% \end{center}
%% \caption[Summary of Models]{Various models implemented in JMOO.  Sorted by Runtime.  
%% Runtimes are averaged over 50 evaluations of arbitrary candidates.}\label{fig:jmoo_models}
%% \end{figure}



% Note that POM3d is not necessarily an agile project in the typical sense, due to the
% non-dynamic nature that typically encompasses agile projects.  It has no place in this study,
% but we include it in the list above to show that POM3 can model any kind of project.  For details
% on how these scenarios are technically defined, see~\fig{POM3abcd}.  To reiterate, POM3d will not be
% used in the analytics of this paper.

% To check that GALE is useful beyond the POM3 MOPs, the experiments of this
% paper are extended to a selection of standard test problems:
% Schaffer, Viennet2, ZDT1, Golinski,
% Srinivas, Tanaka, and Osyczka2.  Refer to~\fig{con} and~\fig{nocon} for their details.



   

\section{Results}\label{sec:exps}

The results address our two research questions:
\bi
\item 
{\bf RQ1 (speed):} Does GALE terminate faster than other MOEA tools?
\item
{\bf RQ2 (quality):} Does GALE return  similar or better
solutions than other MOEA tools?
\ei
To answer these questions,
we ran GALE, NSGA-II, and SPEA2 20 times.
Exception: for CDA, we did not collect
data for 20 runs of NSGA-II \& SPEA2
(since that model ran so slow).
So, for CDA, the results are
averages for 20 runs of GALE and one run
of NSGA-II, SPEA2.

For CDA, runtimes were collected  on  a NASA Linux server with
a 2.4 GHz Intel Core i7 and 8GB of memory.
For other models,
runtimes were measured with Python  running on 
a 2 GHz Intel Core i7 MacBook Air, with 8GB of 1600 MHz DDR3 memory.



\begin{figure}[!b]
%\includegraphics[width=3.5in]{tim/runGale.png}
\begin{center}
\includegraphics[width=2.75in]{barcharts_runtime_galeAbsolute_v2.png}
\end{center}
\caption{GALE, mean runtime in seconds.}\label{fig:runGale} 
\end{figure}


\begin{figure}[!b]
%\includegraphics[width=2in]{tim/runSpear2.png} 
\begin{center}
\noindent
\includegraphics[width=2.25in]{figures/barcharts_runtime_NSGAII_relativeToGale.png}

~\\

\noindent
\includegraphics[width=2.25in]{figures/barcharts_runtime_SPEA2_relativeToGale.png}
\end{center}
\caption{NSGA-II, SPEA2, runtimes, relative to GALE (mean values
over all runs) e.g.,
with SPEA2, ZDT1 ran 1.5 times slower than GALE.}\label{fig:runSpea2} 
\end{figure}



\begin{figure}[!b]
%\includegraphics[width=3.5in]{tim/evals.png}
\includegraphics[width=3.25in]{barcharts_numeval_v2.png}
\caption{Number of evaluations in units of 250 (means over all runs), sorted
by max. number of evaluations.}\label{fig:evals} 
\end{figure}
%% \begin{figure}[!t]
%% \begin{center}
%% \footnotesize
%% \begin{tabular}{|c@{~}|c@{~}|c@{~}|c@{~}|c@{~}|} \hline
%% Model      & NSGA-II       & GALE       & SPEA2       & Compared  \\ \hline \hline
%% Model      & NSGA-II       & GALE       & SPEA2       &  \\ \hline \hline
%% Golinski   & 1,890         & 32         & 1,270       & 49x     \\ \hline
%% Osyczka2   & 2,850         & 33         & 2,720       & 84x     \\ \hline
%% Schaffer   & 1,650         & 44         & 2,340       & 45x     \\ \hline
%% Srinivas   & 1,850         & 44         & 1,420       & 38x     \\ \hline
%% Tanaka     & 1,740         & 45         & 1,530       & 36x     \\ \hline
%% Viennet2   & 2,500         & 48         & 2,730       & 55x     \\ \hline
%% ZDT1       & 1,300         & 47         & 1,190       & 26x     \\ \hline \hline
%% POM3a      & 3,150         & 43         & 3,470       & 78x     \\ \hline
%% POM3b      & 3,550         & 36         & 3,310       & 95x     \\ \hline
%% POM3c      & 3,040         & 53         & 2,760       & 55x     \\ \hline \hline
%% Median     & 2,195         & 44         & 2,530       & 54x     \\ \hline
%% \end{tabular}
%% \end{center}
%% \caption{Number of Evaluations for each MOEA as averaged across all runs for that model.}
%% \label{fig:numeval}
%% \end{figure}
%% \begin{figure}[!t]
%% \begin{center}
%% \footnotesize
%% \begin{tabular}{|c@{~}|c@{~}|c@{~}|c@{~}|} \hline
%% Model      & NSGA-II     & GALE        & SPEA2 \\ \hline \hline
%% Golinski   & 0.7         & 0.8         & 5.8    \\ \hline
%% Osyczka2   & 0.7         & 0.9         & 5.1    \\ \hline
%% Schaffer   & 0.4         & 0.3         & 28.9   \\ \hline
%% Srinivas   & 0.3         & 0.5         & 7.0    \\ \hline
%% Tanaka     & 0.5         & 0.7         & 7.2    \\ \hline
%% Viennet2   & 0.6         & 0.5         & 16.9   \\ \hline
%% ZDT1       & 0.9         & 3.0         & 4.1    \\ \hline \hline
%% POM3a      & 84.7        & 9.7         & 78.4   \\ \hline
%% POM3b      & 6.8         & 2.2         & 20.8   \\ \hline
%% POM3c      & 140.5       & 14.3        & 107.2  \\ \hline \hline
%% Average    & 23.6        & 3.3         & 28.1   \\ \hline 
%% Median     & 0.7         & 0.9         & 12.1   \\ \hline
%% IQR        & 4.7         & 2.3         & 20.8   \\ \hline
%% \end{tabular}
%% \end{center}
%% \caption{CPU RunTime in seconds for each MOEA as averaged across all runs for that model.}
%% \label{fig:runtime}
%% \end{figure}
 

\subsection{Exploring RQ1 (Speed)}

\fig{runGale} shows GALE's runtimes.
Recall that our models form two groups:
the {\em larger models} include XOMO,POM, CDA
and the {\em smaller benchmark models} include  ZDT, Golinski, Water, Viennet2,Two-Bar Truss, Srivinas.
As seen in that figure,
most of the smaller models took two seconds, or less,  to optimize.
On the other hand, the larger models took longer (e.g. CDA needed four minutes).









\fig{runSpea2} compares GALE's runtimes to those
of NSGA-II and SPEA2. In that figure, anything with a relative
runtime over 1.0 ran {\em slower} than GALE. Note that 
GALE was faster than SPEA2 for all models. 

For NSGA-II, GALE was a little slower for the smaller models.
However, when for more complex reasoning, GALE ran much faster. 
For the POM3 models, GALE ran up to an order of magnitude faster than both NSGA-II and SPEA2.
As to CDA, GALE ran two orders of magnitude faster (4 minutes versus 7 hours).



\fig{evals}  shows why GALE  runs so much faster than NSGA-II and SPEA2:
NSGA-II and SPEA2 needed between 1,000 and 4,000 evaluations for each model while GALE terminated after roughly 30 to 50 evaluations.
Across every model, SPEA2 and NSGA-II needed between 25 to 100 times more
evaluations to optimize (mean value: 55 times more evaluations).



\subsection{Exploring RQ2 (Quality)}


\subsubsection{CDA}

The above results show GALE running faster than other MOEAs.
While this seems a useful result, it would be irrelevant if
the quality of the solutions found by GALE were much worse than other MOEAs.


One issue with exploring solution quality
with the very slow models like the CDA model was that NSGA-II and SPEA2 ran so slow that 20 runs would 
require nearly an entire week of CPU.
Hence, in this study NSGA-II and SPEA2 were
only run once on CDA.  \fig{cda} shows quality results for the CDA objectives.
Note that GALE achieved the same (or better) minimizations, after far fewer evalautions, than NSGA-II or SPEA2. 


\begin{figure}
\includegraphics[width=3.3in]{figures/cda.png}
\caption{Execution traces of CDA. X-axis shows number
of evaluations (on a logarithmic scale). 
Solid, colored lines  show  best reductions seen
at each $x$ point.
The y-axis values show percentages of initial values
(so $y=50$ would mean {\em halving} the original value).
For all these
objectives, {\em lower} y-axis values are {\em better}.
}\label{fig:cda} 
\end{figure}


 
\subsubsection{BNH, Golinski, POM3, 
Srinivas, 
Two-Bar Truss, Viennet2, XOMO, and ZDT}

Our other models   were (much)
faster to run.  Hence, for  the other models, we
can offer a more detailed analysis of the quality of
their solutions including hypervolumes and spreads
seen in 20 repeated runs.
\begin{figure}
\includegraphics[width=3.15in]{mathsModelsResults.png}
\caption{Quality results from BNH, Golinski, POM3, Two-Bar Truss, Viennet2, XOMO, ZDT.
All numbers are ratios of mean hypervolumes and spreads achieved in 20 repeated runs of GALE, NSGA-II and SPEA2.
At 100\%, the mean hypervolumes and spreads achieved by GALE are the same as the other optimizers.
In this figure,  {\em better} hypervolumes are {\em larger} while {\em better} spreads are {\em smaller}.}\label{fig:rmodels}
\end{figure}

For example, \fig{rmodels} shows the ratio of mean hypervolumes 
and spreads found in 20 repeated runs of three optimizers. All numbers are ratios of GALE's results divided
by either NSGS-II or SPEA2.  

The Srivinas, POM3c and ZDT2 results were
excluded from \fig{rmodels} after an A12 effect size test
reported a ``small effect'' for the performance deltas between GALE and the other optimizers.
All the other results have the property that $\mathit{A12} \ge 0.6$; i.e. they are not trivially small
differences
  (this A12
  test was recently endorsed by Arcuri and
  Briand at ICSE'11~\cite{arcuri11} as an
  appropriate test to check for trivially small
  differences when studying stochastic processes).

\fig{rmodels} shows that for $\frac{5}{17}$
of the smaller benchmark models (XOMO FL, XOMO GR, and
ZDT346) GALE's hypervolumes were much lower than the
other optimizers.  On the other hand, GALE's
hypervolumes are comparable, or better, for most of
the small benchmark models: \bi
\item
GALE does better than NSGA-II in BNH, POM3a, POM3b and XOMO
O2.
\item As seen in 
\fig{rmodels}, the hypervolumes are very similar for Two-Bar Truss,
Viennet2 and ZDT1.  
\item
Also, as mentioned
above the Srivinas and POM3c and ZDT2 results were only trivially different.
\ei
Other quality indicators offer other evidence for
the
value of GALE's reasoning. For example, \fig{rmodels} shows that GALE
consistently achieves lower and better spreads than the other optimizers.


\begin{figure}[!t]
\scriptsize
\centering
\begin{tabular}{|r|c|c|c|c|} \hline
	&	Model	&	NSGA-II	&	GALE	&	SPEA2	\\ \hline
%DTLZ	&	DTLZ1-d5-o2	&	62\%	&	70\%	&	\colorbox{lightgray}{61\%}	\\
%Toolkit	&	DTLZ1-d5-o4	&	\colorbox{lightgray}{78\%}	&	83\%	&	\colorbox{lightgray}{78\%}	\\
%	&	DTLZ2-d10-o2	&	65\%	&	73\%	&	\colorbox{lightgray}{64\%}	\\
%	&	DTLZ2-d10-o4	&	86\%	&	86\%	&	\colorbox{lightgray}{80\%}	\\
%	&	DTLZ3-d10-o2	&	65\%	&	73\%	&	\colorbox{lightgray}{64\%}	\\
%	&	DTLZ3-d10-o4	&	81\%	&	85\%	&	\colorbox{lightgray}{79\%}	\\
%	&	DTLZ4-d10-o2	&	70\%	&	90\%	&	\colorbox{lightgray}{67\%}	\\
%	&	DTLZ4-d10-o4	&	93\%	&	92\%	&	\colorbox{lightgray}{90\%}	\\
%	&	DTLZ5-d10-o2	&	66\%	&	76\%	&	\colorbox{lightgray}{64\%}	\\
%	&	DTLZ5-d10-o4	&	80\%	&	84\%	&	\colorbox{lightgray}{79\%}	\\
%	&	DTLZ6-d20-o2	&	67\%	&	71\%	&	\colorbox{lightgray}{67\%}	\\
%	&	DTLZ6-d20-o4	&	83\%	&	85\%	&	\colorbox{lightgray}{81\%}	\\ \hline
xomo 	&	xomofl-d27-o4	&	96\%	&	\colorbox{lightgray}{89\%}	&	96\%	\\
models	&	xomogr-d27-o4	&	97\%	&	\colorbox{lightgray}{89\%}	&	97\%	\\
%	&	xomoos-d27-o4	&	97\%	&	\colorbox{lightgray}{88\%}	&	97\%	\\
	&	xomoo2-d27-o4	&	96\%	&	\colorbox{lightgray}{89\%}	&	97\%	\\
%	&	xomoal-d27-o4	&	96\%	&	\colorbox{lightgray}{87\%}	&	96\%	\\ \hline
POM3 	&	POM3A-d9-o4	&	92\%	&	91\%	&	\colorbox{lightgray}{89\%}	\\
models	&	POM3B-d9-o4	&	92\%	&	\colorbox{lightgray}{90\%}	&	\colorbox{lightgray}{89\%}	\\
	&	POM3C-d9-o4	&	96\%	&	\colorbox{lightgray}{94\%}	&	96\%	\\ \hline
Constrained	&	BNH-d2-o2	&	98\%	&	\colorbox{lightgray}{75\%}	&	97\%	\\
%&	Osyczka2-d6-o2	&	77\%	&	\colorbox{lightgray}{69\%}	&	78\%	\\
benchmark&	Srinivas-d2-o2	&	95\%	&	\colorbox{lightgray}{80\%}	&	95\%	\\
%	&	Tanaka-d2-o2	&	\colorbox{lightgray}{84\%}	&	85\%	&	85\%	\\
models	&	TwoBarTruss-d3-o2	&	95\%	&	\colorbox{lightgray}{78\%}	&	95\%	\\
	&	Water-d3-o5	&	95\%	&	\colorbox{lightgray}{90\%}	&	95\%	\\ \hline
Unconstrained	&	Golinski-d7-o2	&	81\%	&	\colorbox{lightgray}{65\%}	&	81\%	\\
benchmark	&	Viennet2-d2-o3	&	\colorbox{lightgray}{73\%}	&	\colorbox{lightgray}{73\%}	&	\colorbox{lightgray}{73\%}	\\
%&	Viennet3-d2-o3	&	88\%	&	\colorbox{lightgray}{78\%}	&	88\%	\\
%	&	Viennet4-d2-o3	&	90\%	&	\colorbox{lightgray}{77\%}	&	89\%	\\
models	&	ZDT1-d30-o2	&	85\%	&	\colorbox{lightgray}{81\%}	&	85\%	\\
	&	ZDT2-d30-o2	&	\colorbox{lightgray}{73\%}	&	\colorbox{lightgray}{72\%}	&	\colorbox{lightgray}{73\%}	\\
	&	ZDT3-d30-o2	&	84\%	&	\colorbox{lightgray}{80\%}	&	85\%	\\
	&	ZDT4-d10-o2	&	\colorbox{lightgray}{68\%}	&	74\%	&	\colorbox{lightgray}{69\%}	\\
	&	ZDT6-d10-o2	&	71\%	&	72\%	&	\colorbox{lightgray}{69\%}	\\ \hline
\end{tabular}
\caption{Median scores comparing final frontier values to
initial populations. Calculated using Equation 1. Lower
scores are better. Gray cells are significantly different
(statistically) and better than the other values in that row.
In the models column, model name shows objectives and decisions;
e.g. d27-o4 means
the model has 27 decisions and 4 objectives.  }
\label{fig:whateveryouwannacallme}
\end{figure}


\begin{figure}
\includegraphics[width=3.25in]{dtlzResults1.png}
\caption{Quality results from DTLZ with (20,40,60,80) decisions and two objectives.
See \fig{rmodels}; i.e. better hypervolumes are {\em larger} while {\em better} spreads are {\em smaller}.}\label{fig:dtlz}
\end{figure}


As to the {\em improvement}, 
 \fig{whateveryouwannacallme} shows the
Equation~1 {\em loss} values between members of the
first and final population generated by different
optimizers. In that figure,
 gray cells are significantly different
(statistically) and better (less is better in that figure)
than the other values in that row
(for statistics, we used Mann-Whitney, 95\%
confidence to test significance, then used A12 to
mark as non-gray any differences that were just
small effects). Note that, in the majority case, GALE's results
are amongst the best for all the optimizers used in this study.


 
\begin{figure*}[!t]
\includegraphics[width=7in]{dtlzResults2.png}
\caption{DTLZ1; d=20, o=2,4,6,8.
Each column is one objective f1,f2,...f8. Colors indicate results
for different optimizers:
GALE results are in 
\textcolor{red}{{\bf RED}},
NSGA-II results are in 
 \textcolor{blue}{{\bf BLUE}},
and the SPEA2 results are shown in 
  \textcolor{darkgreen}{{\bf GREEN}}
(and the 
red,blue, or green lines show the best solution found so far for each
objective for GALE, NSGA-II, and SPEA2 respectively). 
The x-axis of these
plots shows the number of evaluations seen during optimization.
All objective scores are expressed as percentages
of the mean objective scores seen in the baseline population before any optimization
(this baseline is shown as 100\% on the y-axis of all these plots). 
For these objectives, {\em better} scores are {\em smaller}.
}\label{fig:o2468}
\end{figure*}


 



\input{xomoresults}
\begin{figure*}
 \includegraphics[width=6.5in]{figures/figure_analytics_obj_score_plots.png}
\caption{POM results: 20 repeats of each MOEA (one row
per scenario). Same format as \fig{xomoresults}' i.e.
GALE results are in 
\textcolor{red}{{\bf red}}
and NSGA-II results are in 
 \textcolor{blue}{{\bf blue}}. Each x-axis shows number of evaluations (log scale).
On the y-axis, results are expressed
as percentages of the median value seen in the  initial baseline population. 
For all objectives, lower is better and the solid line shows
the best results seen so far on any objective.    }
\label{fig:zdt1objspace}
\end{figure*}




\subsubsection{DTLZ}

 DTLZ  can be configured to include a varying number of decisions
and objectives.  
Our first DTLZ study used two objectives and  {\em changed the number of decisions} from 20 to 80. The other study used twenty decisions and  {\em changed
the number of objectives} from two to eight. We found
 runtime issues with  computing hypervolume for models with many objectives
 so  the  second study  explored one DTLZ model selected at random (DTLZ1).

The results of both studies are shown in \fig{dtlz}
and \fig{o2468}. Note that the differences
between all treatments were not considered ``small'' effects (via A12).

\fig{dtlz} shows results from changing the number of decisions.
GALE's spreads are never much worse than the other
optimizers, and often they are much
better. 
As to the hypervolumes in  \fig{dtlz}:
\bi
\item
A rising ``staircase'' was observed as the number of
objectives was increased. That is,  GALE did {\em better}
as the problem grew more complex (i.e. as the number of decisions increased).
\item
Sometimes, GALE does much better on hypervolumes as
seen in the DTLZ6 results.
\ei
\fig{o2468} shows results from increasing the
number of objectives. 
In those results,   GALE 
finds minimal values for all objectives and does
so using  orders of magnitude fewer evaluations than
 other optimizers.




\subsubsection{Summary of Results from Benchmark Models}

These results from our smaller Benchmark models all show similar trends.
GALE's truncated search sometimes explores a
smaller set of solutions than other
optimizers.  Hence, as one might have expected, GALE's hypervolumes can be smaller than other optimizers. On the other hand,
within the volume it does explore, GALE seems to spread
out more than other optimizers.
Since GALE takes more care to explore its volume of solutions,
it can find better solutions (with most improvement to the objective scores)
than other optimizers.
 

%\begin{figure}
%{\s%% mall
%% \begin{tabular}{r@{~}|r@{~}|r@{~}|r@{~}|r@{~}|r@{~}|r}
%%              & \multicolumn{2}{c}{Hypervolumes}       & \multicolumn{2}{c}{Spreads}           & 
%% \multicolumn{2}{c}{Evaluations}\\	
%%          o    & $\frac{\mathit{gale}}{\mathit{spea2}}$  & $\frac{\mathit{gale}}{\mathit{NSGA-II}}$ & $\frac{\mathit{gale}}{\mathit{spea2}}$ & $\frac{\mathit{gale}}{\mathit{NSGA-II}}$ & 
%% $\frac{\mathit{gale}}{\mathit{spea2}}$ & $\frac{\mathit{gale}}{\mathit{NSGA-II}}$\\\hline
%% 2 & 106       & 111      & 100      & 100      & 4187     & 4202\\
%% 4 & 99        & 106      & 93       & 109      & 3886     & 4072\\
%% 6 & 104       & 106      & 139      & 145      & 3268     & 1712\\
%% 8 & 153       & n/a        & 180      & n/a        & 182      & n.a
%% \end{tabular}
%% }
%% \caption{More quality results from DTLZ with 20 decisions and 





%dtlz d-X0-o2 experiment

%{\scriptsize
%\begin{tabular}{r|r|r|r}
%            algorithm         &  nsga-II&	spea2 &	gale\\\hline
%number of evaluations &  48,260 &	 49,710 &	 928 \\
%evaluations, as a ration of GALE & 53 &	55 & 1
%\end{tabular}}
%% \begin{figure}
%% \begin{center}
%% \footnotesize
%% \begin{tabular}{lc@{~}|c@{~}|c@{~}|c@{~}} 
%% Type&Model      & NSGA-II      & GALE         & SPEA2 \\ \hline 
%% small&Golinski   & 76\%         & \colorbox{lightgray}{68\%}         & 74\%  \\ 
%% &Osyczka2   & 75\%         & \colorbox{lightgray}{72\%}         & 75\%  \\ 
%% &Schaffer   & \colorbox{lightgray}{62\%}         & 63\%         & \colorbox{lightgray}{62\%}  \\ 
%% &Srinivas   & 94\%         & \colorbox{lightgray}{84\%}         & 94\%  \\ 
%% &Tanaka     & 84\%         & \colorbox{lightgray}{75\%}         & 84\%  \\ 
%% &Viennet2   & \colorbox{lightgray}{73\%}         & 75\%         & \colorbox{lightgray}{73\%}  \\
%% &ZDT1       & 87\%         & \colorbox{lightgray}{80\%}         & \colorbox{lightgray}{82\%}  \\ \hline 
%% medium &POM3a      & 92\%         & 91\%         & \colorbox{lightgray}{90\%}  \\
%% &POM3b      & 91\%         & \colorbox{lightgray}{90\%}         & \colorbox{lightgray}{90\%}  \\ 
%% &POM3c      & 93\%         & 90\%         & 93\% 
%% \end{tabular}
%% \end{center}
%% \caption{Median scores comparing final frontier values
%% to initial populations. Calculated using \eq{cdom}.
%% {\em Lower} scores are {\em better}.
%% Gray cells are significantly
%% different (statistically) and better than the other values in that row.}
%% \label{fig:scores}
%% \end{figure}





\subsection{POM3 and XOMO}

\fig{whateveryouwannacallme} showed a statistical comparison of the improvements
achieved between the first and final generations
of GALE, NSGA-II and SPEA2 for the POM3 and XOMO models.
Apart from the statistical analysis, it is also insightful
to look at the changes in the raw objective scores.

\fig{xomoresults} and 
\fig{zdt1objspace} show  how NSGA-II and GALE evolved candidates
with better objective scores for the XOMO and POM3 models.
The format of these figures is the same as \fig{o2468}.
That is,
the y-vertical-axis denotes
changes from the median of the initial population.
Hence, $Y=50$ would indicate that we have halved the value of some objective;
while  $Y \gt 100$ would indicate that optimization failed to improve this objective.

In both \fig{xomoresults} and \fig{zdt1objspace}, all 
the y-axis values are computed such that {\em lower}
values are {\em better}. For example, the results in the column
labeled {\em Incompletion Rate} of \fig{zdt1objspace}
is the ratio {\em initial/now} values. Hence, if we are {\em now} completing a larger
percentage of the requirements, then {\em incompletion} is better if it is
less than 100\%; i.e. \[\mathit{Incompletion}\% = 100 - \mathit{Completion}\%\]


 
\noindent 
In terms of advocating for GALE,
the \fig{xomoresults} results for the XOMO model
are unequivocal: on all dimensions, for all runs of the model,
GALE finds decisions that lead to lower  (i.e.  better) objective scores than NSGA-II.
Further, as shown on the x-axis, GALE does so using far fewer evaluations than NSGA-II.
 

As to the \fig{zdt1objspace} results from POM3, these results are---at first glance---somewhat surprising.
These results seem to say  say that GALE
performed worse than NSGA-II since NSGA-II achieved larger
{\em Cost} reductions. However, the {\em Idle}
results  show otherwise:
 NSGA-II rarely reduced
the {\em Idle} time of the developers  while GALE
found ways to achieve reductions down to near zero percent {\em Idle}.

This observation begs the question: in \fig{zdt1objspace},
how could NSGA-II reduce cost while keeping developers working at the same rate
(i.e. not decrease developer {\em
  Idle} time)?  We checked the model outputs
and realized that  NSGA-II's advice to developers was to
complete
fewer requirements. This is an interesting quirk of pricing models in the agile
community---if developers are rewarded for quickly completing tasks, they will
favor the easier ones, leaving the slower and harder tasks to other developers (who
will get rewarded less). 
Note that this is not necessarily an error in the  POM3 costing routines---providing 
that an optimizer also avoids leaving programmers idle.
In this regard, NSGA-II is far worse than GALE since the latter
successfully reduces {\em cost} as well as the {\em Idle Rate}.




\subsection{Answers to Research Questions}

{\bf RQ1 (speed):} {\em Does GALE terminate faster than other MOEA tools?}:

Note that 
for  smaller models, GALE was slightly slower than NSGA-II (but much faster than 
SPEA2). Also, for large models like CDA, GALE was much faster.
These two effects result from the relative complexity
of (a)~model evaluation versus (b)~GALE's internal clustering of the data.
When model evaluation is very fast, the extra time needed for clustering
dominates the runtimes of GALE.  However, when the
model evaluation is very long, the time
needed for GALE's clustering is dwarfed by the evaluation costs. Hence,
GALE is strongly recommended for models that require long execution times.
Also, even though GALE is slower for smaller models, we would still recommend GALE 
for those small models.
The delta between absolute runtimes of GALE and the other optimizers is negligible 
($\le 3$ seconds).
Further, GALE requires fewer evaluations thus reducing the complexity
for anyone working to understand the reasoning (e.g.  a
programmer conducting system tests on a new model).

{\bf RQ2 (quality):} {\em Does  GALE  return  similar or better solutions than other MOEA tools?}:

GALE's solutions are rarely worse than other optimizers, and sometimes, they are 
better (and note that the generality of this claim is explored futher in \tion{ob}.).


\section{Threats to Validity}


\subsection{Optimizer Bias}\label{sec:ob}
Our reading of 
the 
literature
is that the experimentation in this paper is far larger than
what is typically used
to certify new optimizers.  
Also, we know of no other search-based
SE paper that can achieve GALE's results using so few evaluations.

That said, the applicability of GALE to new models is an open question. We have shown that GALE does better than NSGA-II and SPEA2, for the models
 explored above. This is not to say that we we have not shown
that it works better than {\em all} optimizers over {\em all} data sets. 
 
There are theoretical reasons to conclude that it is impossible
to show that any one optimizer  {\em always} performs best.
Wolpert and Macready~\cite{wolpert97} showed in 1997
that no optimizers necessarily work better than any
other for all possible optimization
problems\footnote{``The computational
  cost of finding a solution, averaged over all
  problems in the class, is the same for any
  solution method.  No solution therefore offers a
  short cut.''~\cite{wolpert97}}. 

In the end, it is honest to just say that our conclusions 
are based on the study that applies a few optimizers to the
22 models explored by GALE in Krall's Ph.D. thesis~\cite{krall14f}
and the 43 models explored later in this paper. For the record these
are:
\bi
\item Small benchmark problems such as those offered in the appendix
(available in the online supplemental material);
\item Larger software process models (XOMO and POM);
\item Very large physics and cognitive models which simulate pilot-automation interaction (CDA).
\ei
At least for these kinds of models, we would recommend GALE.
Also, harking back to \tion{optz}, we would also strongly
endorse GALE for models where:
\bi
\item
The cost of evaluating thousands
(or more)
candidates is prohibitively high;
\item
And the task at hand is functional optimization, which  \tion{optz}
defined as the optimization of models {\em without}  knowledge
of their internal structure.
\ei




\subsection{ Sampling Bias}
This bias threatens any conclusion based on the
analysis of a finite number of optimization
problems.  Hence, even though GALE runs well on
the models studied here, there may well be other
models that could defeat GALE.  

It is very hard to find a representative
sample of models that covers {\em all} kinds of models.
Over the last few decades, there have been many serious attempts
to partition models into different classes, then comment
on how those classes change the complexity of reasoning about those models~\cite{Michie94,Cheeseman91}. To that end, many repositories
now offer instance generators that re-express their model contents
in some canonical form, them offer a service where they can generate
large numbers of mutations of that form. For example, the SPLOT
web site that stores product line models in conjunctive normal form (see goo.gl/n9yZTJ). From that site, researchers can download a tool
that auto-generates a large number of models  with different branching
factors, number of leave features, etc. In this way, it is possible
to generate many similar examples of a particular kind of
model. Unfortunately,  even
when models are as precisely defined as at SPLOT, the variance
in the effort required for their optimization   is  very large~\cite{Hoos02}.
 

For this issue of sampling bias, the best we can do
is define our methods and publicize our tools so
that other researchers can try to repeat our results
and, perhaps, point out a previously unknown bias in
our analysis. Hence, all the experiments (except for
CDA) in this paper are made available online (see \tion{avail}).  Hopefully, other
researchers will emulate our methods to repeat,
refute, or improve our results.


\subsection{Parameter Bias}
For this study, we did not do extensive parameter tuning:
NSGA-II and SPEA2 were run using their default settings
while GALE was run using the settings that worked well on the first model we
studied, which were then frozen for the
rest of this study. As documented above, those parameters were:
\bi
\item $\mu$ = 100:  population size;
\item $\omega$ = $\sqrt{\mu}$: minimum size leaf clusters;
\item $\lambda = 3$: premature stopping criteria (sets the
maximum allowed 
generations without
any improvement on any objective).
\item   $\Delta=1$: the ``accelerator'' that encourages larger mutations;
\item  $\gamma=1.5$: the ``brake'' that blocks excessive mutation.
\ei
(Note that these were constant across all our studies except for the DTLZ models which used
$\Delta=4$).

If this paper was arguing that these parameters were somehow {\em optimal},
then it would be required to present experiments defending the above settings.
However, our  claim is less than that---we only aim  to show
that with these settings, GALE does as well than standard
MOEA tools. In future work, we will explore other  settings.



   
\section{Conclusions}
This paper has introduced GALE, an evolutionary
algorithm that combines active learning with
continuous domination functions and fast spectral
learning to find a response surface model; i.e. a
set of approximations to the Pareto frontier.


We showed for a range of scenarios and models
that GALE found solutions equivalent or better than
standard methods (NSGA-II and SPEA2).  Also, those
solutions were found using one to two orders of
magnitude fewer evaluations.

 
As mentioned above, one repeated result was
that
GALE's truncated search sometimes explores a
smaller set of solutions than other
optimizers: hence, it can sometimes generate lower hypervolumes.
However, for the space it does explore,
GALE seems to do a better job than other optimizers.
A repeated result in the above is that GALE's solutions are more spread out
than other optimizers so 
 it 
can find better solutions (with most improvement to the objective scores).
 

We claim that GALE's superior performance is due to
its better understanding of the shape of the Pareto
frontier.  Standard MOEA tools generate too many
solutions since they explore uninformative parts of
the solution space.  GALE, on the other hand, can
faster find best solutions across that space since
it understands and exploits the shape of the Pareto
frontier.



\section*{Acknowledgements}
 
The authors wish to thank the anonymous reviewers of this
paper for their many excellent suggestions on how to improve this paper.
The work was funded by NSF grant CCF:1017330 and the
Qatar/West Virginia University research grant NPRP
09-12-5-2-470.  This research was partially
conducted at NASA Ames Research Center. 

Reference
herein to any specific commercial product, process,
or service by trade name, trademark, manufacturer,
or otherwise, does not constitute or imply its endorsement by the United States Government.


    

\bibliographystyle{IEEEtran}

\bibliography{references}

 
\begin{IEEEbiography}[{\includegraphics[width=1in,clip,keepaspectratio]{images/joe.png}}]{Joseph Krall}
(Ph.D., WVU)
is a Postdoctoral Research Fellow funded by the National Science Foundation and is employed  at
LoadIQ, a high-tech start-up company in Reno, Nevada that researches and investigates cheaper energy solutions.
His research relates to the application of
intelligent machine learning and data mining algorithms to solve NP-Hard classification problems.
Further research interests lie with multi-objective evolutionary algorithms, 
search based software engineering, games studies, game development, artificial intelligence, and data mining.
\end{IEEEbiography}



\begin{IEEEbiography}[{\includegraphics[width=1in,clip,keepaspectratio]{images/timm.jpg}}]{Tim Menzies} (Ph.D., UNSW)
is a Professor in CS at Morth Carolina State University (USA) and  the author of
over 200 referred publications. In terms of citations, he is one of the top 100 most
most cited
authors in  software engineering (out of 54,000+ researchers, see
http://goo.gl/vggy1). He has been a lead researcher on
projects for NSF, NIJ, DoD, NASA, as well as joint research work with
private companies. He teaches data mining and artificial intelligence
and programming languages. Prof. Menzies is the co-founder of the
PROMISE conference series (along with Jelber Sayyad) devoted to reproducible experiments in
software engineering: see http://openscience.us/repo. He is an
associate editor of IEEE Transactions on Software Engineering. the Empirical Software Engineering Journal, and the
Automated Software Engineering Journal.  For more information, see his web site http://menzies.us
or his vita at http://goo.gl/8eNhY or his list of publications at
http://goo.gl/8KPKA .
\end{IEEEbiography}
 
\begin{IEEEbiography}[{\includegraphics[width=1in,clip,keepaspectratio]{images/davies1_gs.jpg}}]{Misty Davies}
(Ph.D. Stanford),is a Computer Research Engineer at
  NASA Ames Research Center, working within the
  Robust Software Engineering Technical Area.  Her
  work focuses on predicting the behavior of
  complex, engineered systems early in design as a
  way to improve their safety, reliability,
  performance, and cost. Her approach combines
  nascent ideas within systems theory and within the
  mathematics of multi-scale physics modeling.
For more information, see her web site http://ti.arc.nasa.gov/profile/mdavies
or her  list of publications at
http://ti.arc.nasa.gov/profile/mdavies/papers.
\end{IEEEbiography}


\clearpage
 \setcounter{page}{1}
\pagenumbering{Roman}
\section*{Reponse to Reviews}

Dear Prof. Cleland-Huang,


We write to express our thanks to you, and the other reviewers,
for the detailed and helpful feedback on the previous draft. 

Using those notes as a guide,
we have made many  changes all over this document (as detailed below). 
For example, the following sections are either new or extensively rewritten: Section 1 and 2.1, 2.8, 7.1.

Your specific comments are shown below, in bold.

{\bf 1.  As per Reviewer3 - several of the conclusions you draw in this paper are already known and accepted in the more general community.  Please state clearly that you are exploring these claims specifically for the SBSE community.}

(We think in the above sentence, you mean Reviewer4).

Reviewer4 pointed us towards some excellent work on MOEAs and preference-based search. Based on that input, we added section 2.8 (which actually includes some the Reviewer4’s text), including the specification that the work in this paper is local to SE.

Reviewer4 also noted we should remove some overly-general claims and we agreed. Accordingly, the last paragraphs of the Introduction and Conclusion were removed.

{\bf 2.  As per Reviewer3 - some important references are missing and need to be added.}

(We think in the above sentence, you mean Reviewer4).

Reviewer4 offered some excellent suggestions and we have changed/added the references at their suggestion. 
For example, we have  fixed several reference errors that Reviewer4 found in section 2.4.  

{\bf 3.  As per Reviewer3 - question 2 "Does GALE return similar or better solutions than other MOEA tools?" is a rather simplistic question - because it is well known in almost every data mining domain that different algorithms perform better under different circumstances.   While it is OK to ask this question you should (1) constrain the question to the SBSE domain, and (2) broaden it to discuss ``under what circumstances''.}

In section 6 (end) and 7.1 we take care to offer the restrictions you note above. 
For the ``under what circumstances'' message, see the {\em two} dot lists at the end of 7.1

{\bf 4. You need to explain why you are not comparing against the state of the art.  For example, why NSGA-II and not NSGA-III.  Was this a timing issue related to the release of NSGA-III vs. the time at which the work was performed?  This MUST be addressed.  Reviewer1 is not yet satisfied that you have fully explained why you didn't compare to your previous approach - IBEA.}

Regarding ``why did we not use NSGA-III'', to some degree, this was a timing issue. 
NSGA-III only appeared very late in the development of GALE (in fact, at the time of GALE’s first TSE submission, we were unaware of it). In fact, to the best of our knowledge, NSGA-III implementations only started appearing in the standard toolkits in 2015 (see http://goo.gl/x7N9Be)  and there is no NSGA-III implementation in the DEAP toolkit used in this study. 
Now one of my newest grad students plans an NSGA-III implementation in Python but he is having much trouble coding it up from Deb’s pseudocode. Also he is reporting some interesting quirks in the recent C++ implementation of NSGA-III http://goo.gl/gbcWnO that makes us wonder if Deb needs to be much clearer in the NSGA-III description.

That said, there are other issues with NSGA-III:
\bi
\item
NSGA-III was evolved to extend multi-objective (which is usually 2 or 3) to many-objective (10 to 20). If our case is ``GALE was the best many-objective algorithm'', then of course we would have to compare GALE against NSGA-III.
But that is not our case:
GALE is all about reducing the number of evaluations;
it is not about  better handling of many-objective problems.
\item Also,  as we read the literature, the jury is still out on NSGA-III. As we added to this paper:
\begin{quote}
New MOEA  algorithms are being invented all the time.
Late in the development of this project, the authors became
aware of a new version of NSGA-II which, according to its
authors~\cite{6600851}, performed better for large number of objectives.
The merits of this new approach are still be assessed and some
results suggest that, in terms of improving objectives, it is not necessary a superior approach~\cite{sato14}. That said, we know of no similar work
in the SBSE literature that claims anything like GALE's large-scale
reductions in the number
of evaluations.
\end{quote}
\ei
Regarding ``why  did we not compare against IBEA?'',
speaking as one of the authors of the two papers that made IBEA an SBSE ``state of the art algorithm'' (in our ICSE’13 and ASE’13 papers), I think it is a misreading of those papers to declare that the 2004  IBEA algorithm by Zitzler and   Kunzli is now {\bf the} benchmark against which all new algorithms need to be compare. 
\bi
\item
In our ICSE’13 paper we concluded that the continuous domination predicate used by IBEA was a good idea. Now we use continuous domination inside GALE while NSGA-II and SPEA2 use boolean domination.
So for this part of IBEA, we have a used/ do not use comparison.
\item
In our ASE’13 paper, that conclusion was really not about IBEA but about continuous domination and the  PUSH,PULL heuristics   (updating variable bindings after every mutation thus reducing the subsequent search space). Now PUSH,PULL is only relevant for the small number of models were we can access
the dependencies within the internal structures. As Harman says (in our quote on p3), this is not the usual SBSE problem. So while we really like the ASE'13 paper, it is really about a very specific
kind of model.  GALE, on the other hand, is our preferred tool for the more general class of black-box
models
\ei
(By the way, in the new section 2.1 and the suggestion of Reviewer2,  we take great care to specify the range of model types
where we would/ would not use GALE.)





{\bf  5. As per Reviewer2 - several of the images are small/blurry and therefore hard to read.  I realize that you are running into space issues - however each diagram needs to be large enough to read.}

Fig9, Fig10,Fig11 fixed. We are particularly proud of the new fig 11.

{\bf 6. Reviewer 1 still complains about the application to Software Engineering.  However, I believe you have done a good job in showing the applicability to software engineering problems and therefore I am satisfied with this aspect of the paper.  On the other hand - please see my next comment!}

OK
 
{\bf 7. Reviewer 1 complains about the writing - especially that of the introduction.  I have to agree that the introduction is more in the style of a technical report than an introduction to a TSE paper.  Here are some specific suggestions for improvement:

(i) In the opening paragraph explain what you mean by "options of their systems".  Make the problem more concrete for the reader.  You can do this in two sentences and it will help set the context.

(ii) I don't think that process steps listed 1,2,3 work very well for an introduction.  Try to provide a better overview of the novelty of your approach - highlighting what is novel about it but saving the steps of the algorithm for a later section.

(iii)  I'm also personally unconvinced about including Research questions on the first page.  This whole section should be about motivation.  You can explain in words what you want to evaluate and then state the research questions in another section.

(iv) I *particularly* dislike the style of section  1.2.  I understand that you are trying to directly address reviewers concerns - but this isn't the right way to do it.  You shouldn't need to *ask*  the question "What does all this have to do with SE?" because you should have *already* answered it in the introduction.  The problem with your introduction is that you didn't answer it adequately so must recourse to Q\&A.   Questions 2 and 3 are similarly problematic.  So I would sit back and completely redo the introduction.}

Good suggestions- see the  better organized section 1.

As to the over   reviewers  comments, we thank them for their suggestions and have made many changes.
Typos have been fixed; the blurred images have been enhanced.
Also, at the suggestion of Reviewer2,
the decisions and optimization points for these models are now listed at the start of all the subsections of Section4.

Reviewer2 also asked for a simple description of the mathematical optimization problem solved by GALE. That is now at the start of section 2 (and the technical terms in that definition are expanded in that section).

Reviewer3 asks: 
\begin{quote}The argument that you used only one run of CDA because it would take a week to do the 20 runs. With the cheap hardware/software configuration you used, I find it hard to believe that it is not an acceptable/affordable time to spend.
\end{quote}
The thing to note here is that while the other models could be run on local machines, CDA could **only** be run on NASA machines in their supercomputer center. This is a resource that is used, heavily, by the NASA Ames researchers. Also, if there are ever any incidents on current missions, all research is kicked off those supercomputers while they run massive “what if” scenarios on the missions. 

Reviewer4 made many useful suggestions: 
\bi
\item
Thanks to that input, we added some very interesting new citations (in section 2.8) and fixing of some previously incorrect citations.
\item
Reviewer4 also suggested some simplifications that clarified the paper, see points 1,2 above.
\item
They  also suggested we rename our “maths” problems to “benchmark” problems- which we did. Many thanks!
\ei



\newpage

\onecolumn
 \setcounter{page}{1}
\pagenumbering{Roman}

\renewcommand\thefigure{\thesection.\arabic{figure}}   
\setcounter{figure}{0}    

\appendix

\begin{figure}[!h]\scriptsize
%\color{MyDarkBlue}
        \centering
                \begin{tabular}{ | l | c | l@{~} | c@{~} | }
                \hline
                \multicolumn{4}{ | c@{~} | }{Unconstrained Multi-Objective Functions}\\
                \hline
                Name & n & Objectives & Variable Bounds\\\hline\hline
                
                
                
                Golinksi & 7 & \begin{tabular}{ l@{~} }
                {$ r = 0.7854$} \\
                {$ s = 14.933$} \\
                {$ t = 43.0934$} \\
                {$ u = -1.508$} \\
                {$ v = 7.477$} \\
                {$ A(\vec{x}) = r x_1 x_2^2 (\frac{10 x_3^2}{3.0} + s x_3 - t) $}\\
                {$ B(\vec{x}) = u x_1 (x_6^2 + x_7^2)+ v(x_6^3 + x_7^3) + r(x_4*x_6^2 + x_5*x_7^2) $}\\
                {$ aux(\vec{x}) = 745.0 \frac{x_4}{x_2 * x_3} $}\\
                {$ f_1(\vec{x})= A + B $}\\
                {$ f_2(\vec{x})= \frac{\sqrt{aux^2 + 1.69e7}}{0.1 x_6^3} $}
                \end{tabular} & \begin{tabular}{ c@{~} }
                {$ 2.6 <= x_1 <= 3.6 $}\\
                {$ 0.7 <= x_2 <= 0.8 $}\\
                {$ 17.0 <= x_3 <= 28.0 $}\\
                {$ 7.3 <= x_4, x_5 <= 8.3 $}\\
                {$ 2.9 <= x_6 <= 3.9 $}\\
                {$ 5.0 <= x_7 <= 5.5 $} \\
                \end{tabular} \\
                \hline

                Viennet 2 & 2 & \begin{tabular}{ l@{~} }
                {$ f_1(\vec{x})= \frac{(x_1-2)(x_1-2)}{2} + \frac{(x_1+1)(x_1+1)}{13} + 3 $}\\
                {$ f_2(\vec{x})= \frac{(x_1+x_2-3)(x_1+x_2-3)}{36} + \frac{(-x_1+x_2+2)(-x_1+x_2+2)}{8} - 17 $}\\
                {$ f_3(\vec{x})= \frac{(x_1+2x_2-1)(x_1+2x_2-1)}{175} + $} \\ 
                {$ \frac{(2x_2-x_1)(2x_2-x_1)}{17} - 13 $} \\
                \end{tabular} & {$ -4 <= x_i <= 4 $}\\
                \hline
                
                %% Viennet 3 & 2 & \begin{tabular}{ l@{~} }
                %% {$ A(\vec{x}) = 3*x_1 - 2*x_2 + 4        $}\\
                %% {$ B(\vec{x}) = x_1 - x_2 + 1              $}\\
                %% {$ f_1(\vec{x})=  0.5 * (x_1^2 + x_2^2) + sin(x_1^2 + x_2^2)   $}\\
                %% {$ f_2(\vec{x})= \frac{A^2  }{ 8 + \frac{B^2}{27} + 15 }      $}\\
                %% {$ f_3(\vec{x})= \frac{1}{x_1^2 + x_2^2+1} - 1.1 * e^{-(x_1^2) - (x_2^2)}    $} 
                %% \end{tabular} & {$ -3 <= x_i <= 3 $}\\
                %% \hline

                %% Viennet 4 & 2 & \begin{tabular}{ l@{~} }
                %% {$ f_1(\vec{x})= \frac{(x_1-2)(x_1-2)}{2} + \frac{(x_1+1)(x_1+1)}{13} + 3 $}\\
                %% {$ f_2(\vec{x})= \frac{(x_1+x_2-3)(x_1+x_2-3)}{175} + \frac{(2*x_2-x_1)*(2*x_2-x_1)}{17} - 13 $}\\
                %% {$ f_3(\vec{x})= \frac{(3*x_1-2*x_2+4) * ( 3*x_1-2*x_2+4)}{8}   +      $} \\ 
                %% {$ \frac{(x_1-x_2+1)(x_1-x_2+1)}{27} + 15 $} \\
                %% \end{tabular} & {$ -4 <= x_i <= 4 $}\\
                %% \hline

                ZDT1 & 30 & \begin{tabular}{ l@{~} }
                {$ f_1(\vec{x})= x_1 $}\\
                {$ f_2(\vec{x})=  g * (1 - \sqrt{\frac{x_1}{g}})$}\\
                {$ g(\vec{x}) = 1 + \frac{9}{n-1}\sum_{i=2}^{n}(x_i) $}
                \end{tabular} & {$ 0 <= x_i <= 1 $}\\
                \hline

                ZDT2 & 30 & \begin{tabular}{ l@{~} }
                {$ f_1(\vec{x})= x_1 $}\\
                {$ f_2(\vec{x})=  g * (1 - (\frac{x_1}{g})^2)$}\\
                {$ g(\vec{x}) = 1 + \frac{9}{n-1}\sum_{i=2}^{n}(x_i) $}
                \end{tabular} & {$ 0 <= x_i <= 1 $}\\
                \hline
                
                ZDT3 & 30 & \begin{tabular}{ l@{~} }
                {$ f_1(\vec{x})= x_1 $}\\
                {$ f_2(\vec{x})=  g * (1 - \sqrt{(\frac{x_1}{g})} - \frac{x_1}{g} * sin(10*\pi*x_1) )$}\\
                {$ g(\vec{x}) = 1 + \frac{9}{n-1}\sum_{i=2}^{n}(x_i) $}
                \end{tabular} & {$ 0 <= x_i <= 1 $}\\
                \hline
                
                ZDT4 & 10 & \begin{tabular}{ l@{~} }
                {$ f_1(\vec{x})= x_1 $}\\
                {$ f_2(\vec{x})=  g * (1 - \sqrt{(\frac{x_1}{g})} - \frac{x_1}{g} * sin(10*\pi*x_1) )$}\\
                {$ g(\vec{x}) = 1 + 10*(n-1) + \sum_{2}^{n} (x_i^2 - 10*cos(4*\pi*x_i)) $} \\
                \end{tabular} & \begin{tabular}{ c@{~} }
                {$ 0 <= x_1 <= 1 $}\\
                {$ -5 <= x_2,...,x_{10} <= 5$}\\
                \end{tabular}\\
                \hline
                
                ZDT6 & 10 & \begin{tabular}{ l@{~} }
                {$ f_1(\vec{x})=  1 - e^{-4*x_1} * sin(6*\pi*x_1)^6         $}\\
                {$ f_2(\vec{x})=  g * (1  - (\frac{f_1(\vec{x})}{g})^2)  $} \\
                {$ g(\vec{x}) = 1 + 9 * \frac{  \sum_{2}^{n} x_i    }{(n-1)^{0.25}}     $} \\
                \end{tabular} & {$ 0 <= x_i <= 1 $}\\
                \hline


                \end{tabular}
                \caption[Defining the Unconstrained Math Models]{Unconstrained standard benchmark
models. All objectives
are to be minimized unless otherwise denoted.}
        \label{fig:uncon}

\end{figure}


    %\underline{{\bf FINAL SCORING:}}
    %The POM models score a planning policy by comparing a developers' {\em %incremental} decisions
    %to those that might have been made by some omniscient developer (that has access
    %to the   {\em final} cost and priority of requirements).

%\color{MyDarkBlue} 
 
\begin{figure}
\scriptsize
%\color{MyDarkBlue}
        \centering
                \begin{tabular}{ | l | c | l@{~} | l@{~} | c@{~} | }
                        \hline
                        \multicolumn{5}{ | c@{~} | }{Constrained Multi-Objective Functions}\\
                        \hline
                        Name & n & Objectives & Constraints & Variable Bounds\\\hline \hline    
                        
                        BNH & 2 & \begin{tabular}{ l@{~} }
                                {$ f_1(\vec{x}) = 4*x_1^2 + 4x_2^2 $}\\
                                {$ f_2(\vec{x}) = (x_1-5)^2 + (x_2-5)^2 $}\end{tabular} & \begin{tabular}{ l@{~} }
                                {$ g_1(\vec{x}) = ( (x_1-5)^2 + 2*x_2^2) <= 25 $}\\
                                {$ g_2(\vec{x}) = ( (x_1-8)^8 + (x_2 + 3)^2 ) >=7.7 $}\end{tabular} & \begin{tabular}{ c@{~} }
                                {$ 0 <= x_1 <= 5 $}\\
                                {$ 0 <= x_2 <= 3$}\\
                                \end{tabular}\\ \hline
                                
                        %% Osyczka 2 & 6 & \begin{tabular}{ l@{~} }
                        %%         {$ A(\vec{x}) = 25 (x_1 - 2)^2 + (x_2 - 2)^2 $}\\
                        %%         {$ B(\vec{x}) =  (x_3 - 1)^2*(x_4 - 4)^2 +$}\\
                        %%         {$ (x_5 - 2)^2$}\\
                        %%         {$ f_1(\vec{x}) =  0 - A - B  $}\\
                        %%         {$ f_2(\vec{x}) = x_1^2 + x_2^2 + x_3^2 + $}\\
                        %%         {$ x_4^2 + x_5^2 + x_6^2 $} \end{tabular} & \begin{tabular}{ l@{~} }
                        %%         {$ g_1(\vec{x}) = x_1 + x_2 - 2 >= 0 $}\\
                        %%         {$ g_2(\vec{x}) = 6 - x_1 - x_2 >= 0 $}\\
                        %%         {$ g_3(\vec{x}) = 2 - x_2 + x_1 >= 0 $}\\
                        %%         {$ g_4(\vec{x}) = 2 - x_1 + 3 x_2 >= 0 $}\\
                        %%         {$ g_5(\vec{x}) = 4 - (x_3-3)^2 - x_4 >= 0 $}\\
                        %%         {$ g_6(\vec{x}) = (x_5-3)^3 + x_6 - 4 >= 0$}\end{tabular} & \begin{tabular}{ c@{~} }
                        %%                 {$ 0 <= x_1,x_2,x_6 <= 10 $}\\
                        %%                 {$ 1 <= x_3,x_4 <= 5 $}\\
                        %%                 {$ 0 <= x_5 <= 6 $}
                        %%         \end{tabular}\\ \hline
                                
                        Srinivas & 2 & \begin{tabular}{ l@{~} }
                                {$ f_1(\vec{x}) = (x_1 - 2)^2 + (x_2-1)^2 + 2 $}\\
                                {$ f_2(\vec{x}) = 9x_1 - (x_2 -1)^2 $}\end{tabular} & \begin{tabular}{ l@{~} }
                                {$ g_1(\vec{x}) = x_2 + 9x_1 >= 6 $}\\
                                {$ g_2(\vec{x}) = -x_2 + 9x_1 >= 1 $}\end{tabular} & {$ -20 <= x <= 20 $}\\\hline

                        %% Tanaka & 2 & \begin{tabular}{ l@{~} }
                        %%         {$ f_1(\vec{x}) = x_1 $}\\
                        %%         {$ f_2(\vec{x}) = x_2 $}\end{tabular} & \begin{tabular}{ l@{~} }
                        %%         {$ A(\vec{x}) = 0.1 \cos{(16\arctan{(\frac{x_1}{x_2})})} $}\\
                        %%         {$ g_1(\vec{x}) = 1 - x_1^2 - x_2^2 + A <= 0 $}\\
                        %%         {$ g_2(\vec{x}) = (x_1 - 0.5)^2 + $}\\
                        %%         {$ (x_2 - 0.5)^2 <= 0.5 $}\end{tabular} & {$ -\pi <= x <= \pi $}\\\hline
                                
                        Two-bar Truss & 3 & \begin{tabular}{ l@{~} }
                                {$ s_1(\vec{x}) = \frac{20*\sqrt{16+x_3^2}}{(x_1*x_3)}    $} \\
                                {$ s_2(\vec{x}) = \frac{80*\sqrt{1+x_3^2}}{(x_2*x_3)}    $} \\
                                {$ f_1(\vec{x}) = x_1 * \sqrt{16*x_3^2} + x_2*\sqrt{1+x_3^2} $}\\
                                {$ f_2(\vec{x}) = max(s_1, s_2) $}\end{tabular} & \begin{tabular}{ l@{~} }
                                {$ s_1(\vec{x}) = \frac{20*\sqrt{16+x_3^2}}{(x_1*x_3)}    $} \\
                                {$ s_2(\vec{x}) = \frac{80*\sqrt{1+x_3^2}}{(x_2*x_3)}    $} \\
                                {$ g_1(\vec{x}) = ( max(s_1, s_2) ) <= 100000 $} \end{tabular} & \begin{tabular}{ c@{~} }
                                {$ 0 <= x_1,x_2 <= 0.01 $}\\
                                {$ 1 <= x_3 <= 3$}\\
                                \end{tabular}\\ \hline

                        Water & 3 & \begin{tabular}{ l@{~} }
                                {$ f_1(\vec{x}) = 106780.37*(x_2+x_3) + 61704.67$}\\
                                {$ f_2(\vec{x}) = 3000*x_1 $}\\
                                {$ f_3(\vec{x}) = \frac{(305700*2289*x_2)}{((0.06*2289)**0.65)} $}\\
                                {$ E(\vec{x}) = e^{-39.75*x_2+9.9*x_3+2.74} $}\\
                                {$ f_4(\vec{x}) = 250*2289*x_2*E(\vec{x}) $}\\
                                {$ f_5(\vec{x}) = 25*\frac{1.39}{x_1*x_2+4940*x_3-80} $} \end{tabular} & \begin{tabular}{ l@{~} }
                                {$ g_1(\vec{x}) = (   \frac{1 -     0.00139}{x_1*x_2} + 4.94 * x_3 - 0.08  )  $}\\
                                {$ g_2(\vec{x}) = (   \frac{1 -     0.000306}{x_1*x_2} + 1.082 * x_3 - 0.0986 ) $}\\
                                {$ g_3(\vec{x}) = (   \frac{5000 -     12.307}{x_1*x_2} + 4.9408 * x_3 - 4051.02)  $}\\
                                {$ g_4(\vec{x}) = (   \frac{16000 -     2.09}{x_1*x_2} + 804633 * x_3 - 696.71)  $}\\
                                {$ g_5(\vec{x}) = (   \frac{10000 -     2.138}{x_1*x_2} + 7883.39 * x_3 - 705.04)  $}\\
                                {$ g_6(\vec{x}) = (   \frac{2000 -     0.417}{x_1*x_2} + 1721.26 * x_3 - 136.54) $}\\
                                {$ g_7(\vec{x}) = (   \frac{550 -     0.164}{x_1*x_2} + 631.13 * x_3 - 54.48) $}\\
                                {$ g_i(\vec{x}) \ge 0 $}\end{tabular} & \begin{tabular}{ c@{~} }
                                {$ \frac{1}{100} <= x_1 <= \frac{45}{100} $}\\
                                {$ \frac{1}{100} <= x_2,x_3 <= \frac{1}{10}$}\\
                                \end{tabular}\\ \hline
                \end{tabular}
                
                \caption[Defining the Constrained Math Models]{Constrained benchmark models.
All objectives to be minimized unless otherwise denoted.}\label{fig:con}
\end{figure}


\begin{figure}
\scriptsize
%\color{MyDarkBlue}
        \centering
                \begin{tabular}{ | l | c | l@{~} | c@{~} | }
                \hline
                \multicolumn{4}{ | c@{~} | }{DTLZ Family of Models}\\
                \hline
                Name & n & Objectives & Variable Bounds\\\hline\hline
                
                DTLZ1 & n & \begin{tabular}{l@{~} }
                {$ f_1(\vec{x})=  0.5*x_1*x_2*...*x_{M-1}(1+g(x_M))      $}\\
                {$ f_2(\vec{x})=  0.5*x_1*x_2*...*(1 - x_{M-1})(1+g(x_M))      $}\\
                ... \\
                {$ f_{M-1}(\vec{x})=  0.5*x_1*(1 - x_2)(1+g(x_M))      $}\\
                {$ f_M(\vec{x})=  0.5*(1 - x_{1})(1+g(x_M))      $}\\
                {$ g(\vec{x}) = 100 * (  |x_M| + \sum (x_i-0.5)^2 - cos(20\pi(x_i-0.5))    )    $} \\
                \end{tabular} & {$ 0 <= x_i <= 1 $}\\
                \hline

                DTLZ2 & n & \begin{tabular}{l@{~} }
                {$ f_1(\vec{x})=  (1+g(X_M)) cos(x_1*\frac{\pi}{2})...cos(x_{M-1}*\frac{\pi}{2})     $}\\
                {$ f_2(\vec{x})=  (1+g(X_M)) cos(x_1*\frac{\pi}{2})...sin(x_{M-1}*\frac{\pi}{2})     $}\\
                ... \\
                {$ f_{M}(\vec{x})=  (1+g(X_M)) sin(x_1*\frac{\pi}{2})     $}\\
                {$ g(\vec{x}) = \sum (x_i - 0.5)^2     $} \\
                \end{tabular} & {$ 0 <= x_i <= 1 $}\\
                \hline
                
                DTLZ3 & n & \begin{tabular}{l@{~} }
                {$ f_1(\vec{x})=  (1+g(X_M)) cos(x_1*\frac{\pi}{2})...cos(x_{M-1}*\frac{\pi}{2})     $}\\
                {$ f_2(\vec{x})=  (1+g(X_M)) cos(x_1*\frac{\pi}{2})...sin(x_{M-1}*\frac{\pi}{2})     $}\\
                ... \\
                {$ f_{M}(\vec{x})=  (1+g(X_M)) sin(x_1*\frac{\pi}{2})     $}\\
                {$ g(\vec{x}) = 100 * (  |x_M| + \sum (x_i-0.5)^2 - cos(20\pi(x_i-0.5))    )    $} \\
                \end{tabular} & {$ 0 <= x_i <= 1 $}\\
                \hline
                
                DTLZ4 & n & \begin{tabular}{l@{~} }
                {$ f_1(\vec{x})=  (1+g(X_M)) cos(x_1^\alpha*\frac{\pi}{2})...cos(x_{M-1}^\alpha*\frac{\pi}{2})     $}\\
                {$ f_2(\vec{x})=  (1+g(X_M)) cos(x_1^\alpha*\frac{\pi}{2})...sin(x_{M-1}^\alpha*\frac{\pi}{2})     $}\\
                ... \\
                {$ f_{M}(\vec{x})=  (1+g(X_M)) sin(x_1^\alpha*\frac{\pi}{2})     $}\\
                {$ g(\vec{x}) = \sum (x_i - 0.5)^2     $} \\
                \end{tabular} & {$ 0 <= x_i <= 1 $}\\
                \hline
                
                DTLZ5 & n & \begin{tabular}{l@{~} }
                {$ f_1(\vec{x})=  (1+g(X_M)) cos(\theta_1*\frac{\pi}{2})...cos(\theta_{M-1}*\frac{\pi}{2})     $}\\
                {$ f_2(\vec{x})=  (1+g(X_M)) cos(\theta_1*\frac{\pi}{2})...sin(\theta_{M-1}*\frac{\pi}{2})     $}\\
                ... \\
                {$ f_{M}(\vec{x})=  (1+g(X_M)) sin(\theta_1*\frac{\pi}{2})     $}\\
                {$ \theta_i = \frac{\pi}{4(i+g(r))} (1 + 2g(r)x_i)~for~ i = 2,3,...,(M-1) $}\\
                {$ g(\vec{x}) = \sum (x_i - 0.5)^2     $} \\
                \end{tabular} & {$ 0 <= x_i <= 1 $}\\
                \hline
                
                DTLZ6 & n & \begin{tabular}{l@{~} }
                {$ f_1(\vec{x})=  (1+g(X_M)) cos(\theta_1*\frac{\pi}{2})...cos(\theta_{M-1}*\frac{\pi}{2})     $}\\
                {$ f_2(\vec{x})=  (1+g(X_M)) cos(\theta_1*\frac{\pi}{2})...sin(\theta_{M-1}*\frac{\pi}{2})     $}\\
                ... \\
                {$ f_{M}(\vec{x})=  (1+g(X_M)) sin(\theta_1*\frac{\pi}{2})     $}\\
                {$ \theta_i = \frac{\pi}{4(i+g(r))} (1 + 2g(r)x_i)~for~ i = 2,3,...,(M-1) $}\\
                {$ g(\vec{x}) = \sum x_i^{0.1}     $} \\
                \end{tabular} & {$ 0 <= x_i <= 1 $}\\
                \hline

                \end{tabular}
                \caption[Defining the DTLZ Math Models]{DTLZ models. Note: all objectives
to be minimized unless otherwise denoted.}
        \label{fig:dtlzmodels}

\end{figure}




\end{document}
